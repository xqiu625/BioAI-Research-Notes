
## ğŸ“Š Paper Metadata
- **Title:** Diffusion Language Models Are Versatile Protein Learners
- **Authors:** Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu
- **Publication:** PMLR 235, International Conference on Machine Learning (2024)
- **Institution:** ByteDance Research, Nanjing University
- **Code:** https://github.com/bytedance/dplm
- **Tags:** #protein #diffusion #language_models #deep_learning #protein_design

## ğŸ¯ Core Contributions
1. Introduced DPLM (diffusion protein language model), a versatile protein language model combining generative and predictive capabilities
2. Demonstrated strong performance in unconditional protein generation with structural plausibility
3. Showed superior representation learning capabilities compared to existing models
4. Developed multiple conditioning strategies for targeted protein generation

## ğŸ“‹ Paper Structure
### 1. Introduction
- Background: Protein language models are crucial for understanding and designing proteins
- Problem: Current models either excel at prediction (masked LMs) or generation (autoregressive LMs), but not both
- Innovation: Unified framework using discrete diffusion for both tasks

### 2. Methods/Results
- Pre-training on evolutionary-scale protein sequences
- Novel discrete diffusion probabilistic framework
- Three main applications:
  1. Unconditional protein generation
  2. Protein representation learning
  3. Conditional generation with various strategies

### 3. Discussion
- DPLM successfully combines predictive and generative capabilities
- Outperforms existing models in multiple benchmarks
- Demonstrates versatility through various conditioning approaches

## ğŸ”¬ Technical Details
### Framework Components
1. Architecture:
   - Based on Transformer model
   - Model sizes: 150M, 650M, and 3B parameters
   - Discrete diffusion process for protein sequences

2. Key Features:
   - Bidirectional receptive field
   - Non-autoregressive denoising generation
   - Multiple conditioning strategies

## Main Pre-training Dataset
- **Dataset Name**: UniRef50
- **Size**: 
  - ~45 million protein sequences
  - ~14 billion amino acid tokens
- **Processing**:
  - Long sequences truncated to 1024 tokens
  - Random sequence selection for truncation
  - Pre-processed similar to ESM2 protocol

## Downstream Task Evaluation Datasets

### 1. Sequence-Level Tasks
- **Thermostability**
- **HumanPPI** (Protein-Protein Interaction)
- **Metal Ion Binding**
- **EC** (Enzyme Commission Classification)
- **GO** (Gene Ontology) with subtasks:
  - MF (Molecular Function)
  - BP (Biological Process) 
  - CC (Cellular Component)

### 2. Localization Prediction
- **DeepLoc**:
  - Subcellular localization
  - Binary classification

### 3. Structure Prediction
- **CASP12** for secondary structure prediction
- **TAPE** dataset for linear probing of secondary structure

### 4. Inverse Folding Datasets
- **CATH 4.2**
- **CATH 4.3**
Used for evaluating structure-conditioned sequence generation

### 5. Motif-Scaffolding Evaluation
- 17 motif-scaffolding problems
- Each problem sampled 100 sequences
- Success criteria:
  - motif-RMSD < 1Ã…
  - overall pLDDT > 70

### Reference Datasets
- **PDB** (Protein Data Bank):
  - Used for structural novelty evaluation
  - Comparison of structural characteristics
- **UniRef50 subset**:
  - Used as reference for natural sequences
  - Matched length sampling for comparisons

## Dataset Usage Details

### For Structure Evaluation
Used multiple tools for structure prediction and evaluation:
- **ESMFold**: Structure prediction
- **OmegaFold**: Alternative structure prediction tool
- Metrics calculated:
  - pLDDT scores
  - TM-scores
  - Secondary structure statistics

### For Training Configuration
- Batch sizes:
  - 320K tokens for 150M model
  - 1M tokens for 650M/3B models
- Training steps: 100K updates
- Sequence length limit: 1024 tokens


### Implementation
- Pre-training Data: UniRef50 database (~45 million protein sequences)
- Maximum sequence length: 1024 tokens
- Training duration: 100K updates
- Batch sizes: 320K for 150M model, 1M for larger models

## ğŸ“Š Evaluation
### Metrics
1. Structure Evaluation:
   - pLDDT scores
   - TM-scores
   - Secondary structure statistics

2. Performance Metrics:
   - Foldability
   - Novelty
   - Diversity
   - Downstream task performance

### Results
- Achieved high pLDDT scores (>80) for generated proteins
- Superior performance in downstream tasks compared to ESM-2
- Successful conditional generation in multiple scenarios

## ğŸ’­ Critical Analysis
### Strengths
1. Unified framework for both generation and prediction
2. Strong performance across multiple tasks
3. Versatile conditioning capabilities

### Limitations
1. Computational requirements for large models
2. Some tasks still benefit from sequence-only approaches

### Future Directions
1. Integration with protein structure modeling
2. Extension to longer biological sequences
3. Incorporation of wet-lab experimental feedback

## ğŸ“Œ Key Takeaways
1. Discrete diffusion is effective for protein language modeling
2. Combined generative-predictive capabilities are achievable
3. Multiple conditioning strategies enable targeted protein design

## ğŸ“š Related Work
- ESM family: Previous state-of-the-art in protein representation learning
- EvoDiff: Related work in protein sequence generation
- RFDiffusion: Structure-based protein design

## ğŸ’¡ Personal Notes
### Novel Aspects
- First successful application of discrete diffusion to protein language modeling
- Creative combination of representation learning and generation
- Innovative conditioning strategies for targeted design

# ğŸ¯ è®ºæ–‡æ ¸å¿ƒè´¡çŒ®
1. æå‡ºäº† **DPLMï¼ˆDiffusion Protein Language Modelï¼‰** â€”â€” ä¸€ç§ç»“åˆäº†**ç”Ÿæˆ**å’Œ**é¢„æµ‹**èƒ½åŠ›çš„å¤šåŠŸèƒ½è›‹ç™½è¯­è¨€æ¨¡å‹ã€‚
2. åœ¨ **æ— æ¡ä»¶è›‹ç™½ç”Ÿæˆ** æ–¹é¢è¡¨ç°å¼ºåŠ² â€”â€” ç”Ÿæˆçš„è›‹ç™½åºåˆ—åœ¨ç»“æ„ä¸Šå…·æœ‰åˆç†æ€§ã€‚
3. **ç›¸æ¯”ç°æœ‰æ¨¡å‹ï¼Œå±•ç°æ›´ä¼˜è¶Šçš„è¡¨å¾å­¦ä¹ èƒ½åŠ›** â€”â€” åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå‡è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚
4. **å¼€å‘äº†å¤šç§æ¡ä»¶æ§åˆ¶ç­–ç•¥** â€”â€” å¯ä»¥è¿›è¡Œå®šå‘çš„è›‹ç™½ç”Ÿæˆã€‚

---

# ğŸ“‹ è®ºæ–‡ç»“æ„

## 1. å¼•è¨€
- **èƒŒæ™¯**ï¼š è›‹ç™½è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰åœ¨è›‹ç™½ç†è§£å’Œè®¾è®¡ä¸­è‡³å…³é‡è¦ã€‚
- **é—®é¢˜**ï¼š ç°æœ‰æ¨¡å‹é€šå¸¸**åªèƒ½åœ¨é¢„æµ‹ï¼ˆæ©ç è¯­è¨€æ¨¡å‹ï¼Œå¦‚ ESMï¼‰æˆ–ç”Ÿæˆï¼ˆè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œå¦‚ ProtGPTï¼‰ä¸­è¡¨ç°è‰¯å¥½**ï¼Œæ— æ³•å…¼é¡¾ä¸¤è€…ã€‚
- **åˆ›æ–°ç‚¹**ï¼š è¯¥ç ”ç©¶é‡‡ç”¨ **ç¦»æ•£æ‰©æ•£ï¼ˆdiscrete diffusionï¼‰** æ–¹æ³•ï¼Œæå‡º**ç»Ÿä¸€æ¡†æ¶**ï¼Œå…¼é¡¾**é¢„æµ‹**ä¸**ç”Ÿæˆ**ä»»åŠ¡ã€‚

## 2. æ–¹æ³•ä¸ç»“æœ
- è®­ç»ƒäº**è¿›åŒ–çº§åˆ«çš„è›‹ç™½åºåˆ—**ï¼ˆUniRef50 æ•°æ®é›†ï¼‰ã€‚
- é‡‡ç”¨ **ç¦»æ•£æ‰©æ•£æ¦‚ç‡æ¡†æ¶ï¼ˆDiscrete Diffusion Probabilistic Model, DDPMï¼‰**ï¼š
  - å°†è›‹ç™½åºåˆ—æ˜ å°„åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­çš„ä¸åŒå™ªå£°çº§åˆ«ã€‚
  - é€šè¿‡å»å™ªæ­¥éª¤ï¼Œé€æ­¥è¿˜åŸå‡ºçœŸå®çš„è›‹ç™½åºåˆ—ã€‚
- **ä¸»è¦åº”ç”¨**ï¼š
  1. æ— æ¡ä»¶è›‹ç™½ç”Ÿæˆ
  2. è›‹ç™½è¡¨å¾å­¦ä¹ 
  3. åŸºäºæ¡ä»¶æ§åˆ¶çš„å®šå‘è›‹ç™½ç”Ÿæˆ

## 3. è®¨è®º
- DPLM **æˆåŠŸç»“åˆäº†ç”Ÿæˆå’Œé¢„æµ‹èƒ½åŠ›**ã€‚
- åœ¨**å¤šé¡¹åŸºå‡†ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ¨¡å‹**ã€‚
- **å±•ç¤ºäº†å¤šç§æ¡ä»¶æ§åˆ¶æ–¹æ³•**ï¼Œå¦‚ï¼š
  - **åºåˆ—çº¦æŸ**ï¼ˆæä¾›éƒ¨åˆ†åºåˆ—ï¼Œè¡¥å…¨ç¼ºå¤±éƒ¨åˆ†ï¼‰ã€‚
  - **åŠŸèƒ½çº¦æŸ**ï¼ˆé™åˆ¶ç”Ÿæˆè›‹ç™½çš„ç”Ÿç‰©å­¦åŠŸèƒ½ï¼‰ã€‚
  - **ç»“æ„çº¦æŸ**ï¼ˆåŸºäºè›‹ç™½ç»“æ„è¿›è¡Œå®šå‘ç”Ÿæˆï¼‰ã€‚

---

# ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

## æ¨¡å‹æ¶æ„
### 1. åŸºç¡€æ¶æ„ï¼š
- **Transformer-based ç»“æ„**
- **ä¸‰ç§æ¨¡å‹è§„æ¨¡**ï¼š
  - **150M** å‚æ•°
  - **650M** å‚æ•°
  - **3B** å‚æ•°
- **é‡‡ç”¨ç¦»æ•£æ‰©æ•£è¿‡ç¨‹å¯¹è›‹ç™½åºåˆ—è¿›è¡Œå»ºæ¨¡**ã€‚

### 2. å…³é”®ç‰¹æ€§ï¼š
- **åŒå‘æ„Ÿå—ï¼ˆBidirectional receptive fieldï¼‰**ï¼Œé¿å…äº†è‡ªå›å½’æ¨¡å‹çš„ä¿¡æ¯å±è”½é—®é¢˜ã€‚
- **éè‡ªå›å½’å»å™ªç”Ÿæˆï¼ˆNon-autoregressive denoising generationï¼‰**ï¼Œç›¸æ¯”ä¼ ç»Ÿè‡ªå›å½’æ–¹æ³•æ›´é«˜æ•ˆã€‚
- **å¤šç§æ¡ä»¶æ§åˆ¶ç­–ç•¥**ï¼Œå¯ä»¥åœ¨ä¸åŒçº¦æŸä¸‹è¿›è¡Œè›‹ç™½è®¾è®¡ã€‚

---

# ğŸ§¬ è®­ç»ƒæ•°æ®é›†

## ä¸»é¢„è®­ç»ƒæ•°æ®é›†
- **æ•°æ®é›†åç§°**ï¼š UniRef50
- **è§„æ¨¡**ï¼š
  - çº¦ **4500 ä¸‡æ¡** è›‹ç™½åºåˆ—
  - çº¦ **140 äº¿** æ°¨åŸºé…¸ token
- **é¢„å¤„ç†**ï¼š
  - **é•¿åºåˆ—æˆªæ–­è‡³ 1024 ä¸ªæ°¨åŸºé…¸ token**
  - **éšæœºé€‰æ‹©å­åºåˆ—è¿›è¡Œæˆªæ–­**
  - **é‡‡ç”¨ä¸ ESM-2 ç±»ä¼¼çš„é¢„å¤„ç†åè®®**

---

# ğŸ¯ è¯„ä¼°æ•°æ®é›†

## 1. åºåˆ—çº§ä»»åŠ¡
- **è›‹ç™½çƒ­ç¨³å®šæ€§é¢„æµ‹ï¼ˆThermostabilityï¼‰**
- **è›‹ç™½-è›‹ç™½ç›¸äº’ä½œç”¨é¢„æµ‹ï¼ˆHumanPPIï¼‰**
- **é‡‘å±ç¦»å­ç»“åˆï¼ˆMetal Ion Bindingï¼‰**
- **é…¶åˆ†ç±»ï¼ˆECï¼‰**
- **åŸºå› æœ¬ä½“ï¼ˆGOï¼‰ ä»»åŠ¡**ï¼š
  - **åˆ†å­åŠŸèƒ½ï¼ˆMFï¼‰**
  - **ç”Ÿç‰©è¿‡ç¨‹ï¼ˆBPï¼‰**
  - **ç»†èƒç»„åˆ†ï¼ˆCCï¼‰**

## 2. äºšç»†èƒå®šä½é¢„æµ‹
- **DeepLoc**ï¼š
  - é¢„æµ‹è›‹ç™½çš„**äºšç»†èƒå®šä½**
  - **äºŒåˆ†ç±»ä»»åŠ¡**

## 3. ç»“æ„é¢„æµ‹
- **CASP12**ï¼šç”¨äºè›‹ç™½äºŒçº§ç»“æ„é¢„æµ‹
- **TAPE æ•°æ®é›†**ï¼šç”¨äºçº¿æ€§æ¢æµ‹ï¼ˆlinear probingï¼‰äºŒçº§ç»“æ„ä¿¡æ¯

## 4. é€†æŠ˜å ï¼ˆInverse Foldingï¼‰
- **CATH 4.2 & CATH 4.3**
  - ç”¨äºè¯„ä¼°**ç»“æ„-åºåˆ—æ˜ å°„èƒ½åŠ›**
  - é€‚ç”¨äº**ç»“æ„çº¦æŸçš„è›‹ç™½è®¾è®¡ä»»åŠ¡**

## 5. ç»“æ„ä¿®é¥°ï¼ˆMotif-Scaffoldingï¼‰
- **17 ä¸ª Motif-Scaffolding ä»»åŠ¡**
- æ¯ä¸ªä»»åŠ¡**éšæœºé‡‡æ · 100 æ¡åºåˆ—**
- **è¯„åˆ¤æ ‡å‡†**ï¼š
  - **Motif-RMSD < 1Ã…**
  - **pLDDT > 70**

## 6. å‚è€ƒæ•°æ®é›†
- **PDBï¼ˆè›‹ç™½æ•°æ®åº“ï¼‰**ï¼š
  - è¯„ä¼°**ç”Ÿæˆè›‹ç™½çš„ç»“æ„æ–°é¢–æ€§**
  - é€šè¿‡ä¸å·²çŸ¥è›‹ç™½çš„å¯¹æ¯”ï¼Œæµ‹é‡å…¶**ç»“æ„å¤šæ ·æ€§**
- **UniRef50 å­é›†**ï¼š
  - ä½œä¸º**å¤©ç„¶åºåˆ—çš„å‚è€ƒ**
  - åœ¨**ç›¸åŒé•¿åº¦èŒƒå›´å†…è¿›è¡Œæ¯”è¾ƒ**

---

# ğŸ” è®­ç»ƒé…ç½®
- **Batch Size**ï¼š
  - **150M æ¨¡å‹**ï¼š**320K tokens**
  - **650M/3B æ¨¡å‹**ï¼š**1M tokens**
- **è®­ç»ƒæ­¥æ•°**ï¼š**10 ä¸‡æ¬¡**
- **æœ€å¤§åºåˆ—é•¿åº¦**ï¼š**1024 tokens**
- **ç»“æ„è¯„ä¼°å·¥å…·**ï¼š
  - **ESMFold**ï¼ˆç”¨äºç»“æ„é¢„æµ‹ï¼‰
  - **OmegaFold**ï¼ˆç”¨äºç»“æ„é¢„æµ‹ï¼‰
- **å…³é”®æŒ‡æ ‡**ï¼š
  - **pLDDT åˆ†æ•°**ï¼ˆé¢„æµ‹è´¨é‡ï¼‰
  - **TM-score**ï¼ˆç»“æ„ç›¸ä¼¼æ€§ï¼‰
  - **äºŒçº§ç»“æ„ç»Ÿè®¡**

---

# ğŸ’­ å…³é”®åˆ†æ

## **ä¼˜åŠ¿**
1. **é¦–æ¬¡å°†ç¦»æ•£æ‰©æ•£åº”ç”¨äºè›‹ç™½è¯­è¨€æ¨¡å‹**
2. **åŒæ—¶å…·å¤‡ç”Ÿæˆå’Œé¢„æµ‹èƒ½åŠ›**
3. **çµæ´»çš„æ¡ä»¶æ§åˆ¶èƒ½åŠ›**
4. **åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ¨¡å‹**

## **å±€é™æ€§**
1. **è®­ç»ƒæˆæœ¬é«˜**ï¼ˆè®¡ç®—éœ€æ±‚å¤§ï¼‰
2. **éƒ¨åˆ†ä»»åŠ¡ä»ç„¶ä¾èµ–åºåˆ—ä¿¡æ¯ï¼Œä¸ä¸€å®šéœ€è¦æ‰©æ•£æ–¹æ³•**

 
