
## 📊 Paper Metadata
- **Title:** Diffusion Language Models Are Versatile Protein Learners
- **Authors:** Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu
- **Publication:** PMLR 235, International Conference on Machine Learning (2024)
- **Institution:** ByteDance Research, Nanjing University
- **Code:** https://github.com/bytedance/dplm
- **Tags:** #protein #diffusion #language_models #deep_learning #protein_design

## 🎯 Core Contributions
1. Introduced DPLM (diffusion protein language model), a versatile protein language model combining generative and predictive capabilities
2. Demonstrated strong performance in unconditional protein generation with structural plausibility
3. Showed superior representation learning capabilities compared to existing models
4. Developed multiple conditioning strategies for targeted protein generation

## 📋 Paper Structure
### 1. Introduction
- Background: Protein language models are crucial for understanding and designing proteins
- Problem: Current models either excel at prediction (masked LMs) or generation (autoregressive LMs), but not both
- Innovation: Unified framework using discrete diffusion for both tasks

### 2. Methods/Results
- Pre-training on evolutionary-scale protein sequences
- Novel discrete diffusion probabilistic framework
- Three main applications:
  1. Unconditional protein generation
  2. Protein representation learning
  3. Conditional generation with various strategies

### 3. Discussion
- DPLM successfully combines predictive and generative capabilities
- Outperforms existing models in multiple benchmarks
- Demonstrates versatility through various conditioning approaches

## 🔬 Technical Details
### Framework Components
1. Architecture:
   - Based on Transformer model
   - Model sizes: 150M, 650M, and 3B parameters
   - Discrete diffusion process for protein sequences

2. Key Features:
   - Bidirectional receptive field
   - Non-autoregressive denoising generation
   - Multiple conditioning strategies

## Main Pre-training Dataset
- **Dataset Name**: UniRef50
- **Size**: 
  - ~45 million protein sequences
  - ~14 billion amino acid tokens
- **Processing**:
  - Long sequences truncated to 1024 tokens
  - Random sequence selection for truncation
  - Pre-processed similar to ESM2 protocol

## Downstream Task Evaluation Datasets

### 1. Sequence-Level Tasks
- **Thermostability**
- **HumanPPI** (Protein-Protein Interaction)
- **Metal Ion Binding**
- **EC** (Enzyme Commission Classification)
- **GO** (Gene Ontology) with subtasks:
  - MF (Molecular Function)
  - BP (Biological Process) 
  - CC (Cellular Component)

### 2. Localization Prediction
- **DeepLoc**:
  - Subcellular localization
  - Binary classification

### 3. Structure Prediction
- **CASP12** for secondary structure prediction
- **TAPE** dataset for linear probing of secondary structure

### 4. Inverse Folding Datasets
- **CATH 4.2**
- **CATH 4.3**
Used for evaluating structure-conditioned sequence generation

### 5. Motif-Scaffolding Evaluation
- 17 motif-scaffolding problems
- Each problem sampled 100 sequences
- Success criteria:
  - motif-RMSD < 1Å
  - overall pLDDT > 70

### Reference Datasets
- **PDB** (Protein Data Bank):
  - Used for structural novelty evaluation
  - Comparison of structural characteristics
- **UniRef50 subset**:
  - Used as reference for natural sequences
  - Matched length sampling for comparisons

## Dataset Usage Details

### For Structure Evaluation
Used multiple tools for structure prediction and evaluation:
- **ESMFold**: Structure prediction
- **OmegaFold**: Alternative structure prediction tool
- Metrics calculated:
  - pLDDT scores
  - TM-scores
  - Secondary structure statistics

### For Training Configuration
- Batch sizes:
  - 320K tokens for 150M model
  - 1M tokens for 650M/3B models
- Training steps: 100K updates
- Sequence length limit: 1024 tokens


### Implementation
- Pre-training Data: UniRef50 database (~45 million protein sequences)
- Maximum sequence length: 1024 tokens
- Training duration: 100K updates
- Batch sizes: 320K for 150M model, 1M for larger models

## 📊 Evaluation
### Metrics
1. Structure Evaluation:
   - pLDDT scores
   - TM-scores
   - Secondary structure statistics

2. Performance Metrics:
   - Foldability
   - Novelty
   - Diversity
   - Downstream task performance

### Results
- Achieved high pLDDT scores (>80) for generated proteins
- Superior performance in downstream tasks compared to ESM-2
- Successful conditional generation in multiple scenarios

## 💭 Critical Analysis
### Strengths
1. Unified framework for both generation and prediction
2. Strong performance across multiple tasks
3. Versatile conditioning capabilities

### Limitations
1. Computational requirements for large models
2. Some tasks still benefit from sequence-only approaches

### Future Directions
1. Integration with protein structure modeling
2. Extension to longer biological sequences
3. Incorporation of wet-lab experimental feedback

## 📌 Key Takeaways
1. Discrete diffusion is effective for protein language modeling
2. Combined generative-predictive capabilities are achievable
3. Multiple conditioning strategies enable targeted protein design

## 📚 Related Work
- ESM family: Previous state-of-the-art in protein representation learning
- EvoDiff: Related work in protein sequence generation
- RFDiffusion: Structure-based protein design

## 💡 Personal Notes
### Novel Aspects
- First successful application of discrete diffusion to protein language modeling
- Creative combination of representation learning and generation
- Innovative conditioning strategies for targeted design

# 🎯 论文核心贡献
1. 提出了 **DPLM（Diffusion Protein Language Model）** —— 一种结合了**生成**和**预测**能力的多功能蛋白语言模型。
2. 在 **无条件蛋白生成** 方面表现强劲 —— 生成的蛋白序列在结构上具有合理性。
3. **相比现有模型，展现更优越的表征学习能力** —— 在多个基准任务上均超越现有模型。
4. **开发了多种条件控制策略** —— 可以进行定向的蛋白生成。

---

# 📋 论文结构

## 1. 引言
- **背景**： 蛋白语言模型（PLM）在蛋白理解和设计中至关重要。
- **问题**： 现有模型通常**只能在预测（掩码语言模型，如 ESM）或生成（自回归语言模型，如 ProtGPT）中表现良好**，无法兼顾两者。
- **创新点**： 该研究采用 **离散扩散（discrete diffusion）** 方法，提出**统一框架**，兼顾**预测**与**生成**任务。

## 2. 方法与结果
- 训练于**进化级别的蛋白序列**（UniRef50 数据集）。
- 采用 **离散扩散概率框架（Discrete Diffusion Probabilistic Model, DDPM）**：
  - 将蛋白序列映射到扩散过程中的不同噪声级别。
  - 通过去噪步骤，逐步还原出真实的蛋白序列。
- **主要应用**：
  1. 无条件蛋白生成
  2. 蛋白表征学习
  3. 基于条件控制的定向蛋白生成

## 3. 讨论
- DPLM **成功结合了生成和预测能力**。
- 在**多项基准任务上超越现有模型**。
- **展示了多种条件控制方法**，如：
  - **序列约束**（提供部分序列，补全缺失部分）。
  - **功能约束**（限制生成蛋白的生物学功能）。
  - **结构约束**（基于蛋白结构进行定向生成）。

---

# 🔬 技术细节

## 模型架构
### 1. 基础架构：
- **Transformer-based 结构**
- **三种模型规模**：
  - **150M** 参数
  - **650M** 参数
  - **3B** 参数
- **采用离散扩散过程对蛋白序列进行建模**。

### 2. 关键特性：
- **双向感受（Bidirectional receptive field）**，避免了自回归模型的信息屏蔽问题。
- **非自回归去噪生成（Non-autoregressive denoising generation）**，相比传统自回归方法更高效。
- **多种条件控制策略**，可以在不同约束下进行蛋白设计。

---

# 🧬 训练数据集

## 主预训练数据集
- **数据集名称**： UniRef50
- **规模**：
  - 约 **4500 万条** 蛋白序列
  - 约 **140 亿** 氨基酸 token
- **预处理**：
  - **长序列截断至 1024 个氨基酸 token**
  - **随机选择子序列进行截断**
  - **采用与 ESM-2 类似的预处理协议**

---

# 🎯 评估数据集

## 1. 序列级任务
- **蛋白热稳定性预测（Thermostability）**
- **蛋白-蛋白相互作用预测（HumanPPI）**
- **金属离子结合（Metal Ion Binding）**
- **酶分类（EC）**
- **基因本体（GO） 任务**：
  - **分子功能（MF）**
  - **生物过程（BP）**
  - **细胞组分（CC）**

## 2. 亚细胞定位预测
- **DeepLoc**：
  - 预测蛋白的**亚细胞定位**
  - **二分类任务**

## 3. 结构预测
- **CASP12**：用于蛋白二级结构预测
- **TAPE 数据集**：用于线性探测（linear probing）二级结构信息

## 4. 逆折叠（Inverse Folding）
- **CATH 4.2 & CATH 4.3**
  - 用于评估**结构-序列映射能力**
  - 适用于**结构约束的蛋白设计任务**

## 5. 结构修饰（Motif-Scaffolding）
- **17 个 Motif-Scaffolding 任务**
- 每个任务**随机采样 100 条序列**
- **评判标准**：
  - **Motif-RMSD < 1Å**
  - **pLDDT > 70**

## 6. 参考数据集
- **PDB（蛋白数据库）**：
  - 评估**生成蛋白的结构新颖性**
  - 通过与已知蛋白的对比，测量其**结构多样性**
- **UniRef50 子集**：
  - 作为**天然序列的参考**
  - 在**相同长度范围内进行比较**

---

# 🔍 训练配置
- **Batch Size**：
  - **150M 模型**：**320K tokens**
  - **650M/3B 模型**：**1M tokens**
- **训练步数**：**10 万次**
- **最大序列长度**：**1024 tokens**
- **结构评估工具**：
  - **ESMFold**（用于结构预测）
  - **OmegaFold**（用于结构预测）
- **关键指标**：
  - **pLDDT 分数**（预测质量）
  - **TM-score**（结构相似性）
  - **二级结构统计**

---

# 💭 关键分析

## **优势**
1. **首次将离散扩散应用于蛋白语言模型**
2. **同时具备生成和预测能力**
3. **灵活的条件控制能力**
4. **在多个基准任务上优于现有模型**

## **局限性**
1. **训练成本高**（计算需求大）
2. **部分任务仍然依赖序列信息，不一定需要扩散方法**

 
