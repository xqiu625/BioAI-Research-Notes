## ğŸ“Š Paper Metadata
- **Title**: Nucleotide Transformer: building and evaluating robust foundation models for human genomics
- **Authors**: Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo P. de Almeida, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir, Marie Lopez, Thomas Pierrot
- **Publication**: Nature Methods
- **Institution**: InstaDeep, NVIDIA, Technical University of Munich
- **Paper Link**: https://doi.org/10.1038/s41592-024-02523-z
- **Code/Data**: https://github.com/instadeepai/nucleotide-transformer
- **Date Read**: 2025-02-18

## ğŸ”„ Key Scientific Insights

### 1. Conceptual Innovation
- Development of transformer-based foundation models (Nucleotide Transformer) pre-trained on DNA sequences
- Models can learn context-specific representations of nucleotide sequences without task-specific supervision
```
è¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¸“é—¨è®­ç»ƒï¼ˆæ²¡æœ‰ä»»åŠ¡ç‰¹å®šç›‘ç£ï¼‰çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ åˆ°æ ¸è‹·é…¸åºåˆ—çš„ä¸Šä¸‹æ–‡ç‰¹å®šè¡¨ç¤ºã€‚

è¯¦ç»†è§£é‡Šï¼š

1. **æ— éœ€ä»»åŠ¡ç‰¹å®šç›‘ç£**ï¼šä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡å¸¦æ ‡ç­¾çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹å®Œæˆç‰¹å®šä»»åŠ¡ï¼ˆå¦‚é¢„æµ‹å¯åŠ¨å­ã€å¢å¼ºå­æˆ–å‰ªæ¥ä½ç‚¹ï¼‰ã€‚è¿™ç§æ–¹æ³•è¢«ç§°ä¸º"æœ‰ç›‘ç£å­¦ä¹ "ï¼Œå› ä¸ºæ¨¡å‹æ˜¯åœ¨æ˜ç¡®çŸ¥é“"æ­£ç¡®ç­”æ¡ˆ"çš„æƒ…å†µä¸‹å­¦ä¹ çš„ã€‚

2. **ä¸Šä¸‹æ–‡ç‰¹å®šè¡¨ç¤º**ï¼šè¿™äº›æ¨¡å‹èƒ½å¤Ÿç†è§£DNAåºåˆ—ä¸­æ ¸è‹·é…¸çš„ä¸Šä¸‹æ–‡å«ä¹‰ã€‚ä¾‹å¦‚ï¼ŒåŒä¸€ä¸ªæ ¸è‹·é…¸åºåˆ—åœ¨ä¸åŒåŸºå› ç¯å¢ƒä¸­å¯èƒ½æœ‰ä¸åŒçš„åŠŸèƒ½ï¼Œæ¨¡å‹å¯ä»¥æ•æ‰åˆ°è¿™ç§å·®å¼‚ã€‚è¿™äº›è¡¨ç¤ºï¼ˆä¹Ÿç§°ä¸ºåµŒå…¥æˆ–embeddingsï¼‰ç¼–ç äº†åºåˆ—çš„ç”Ÿç‰©å­¦ç‰¹æ€§å’ŒåŠŸèƒ½ä¿¡æ¯ã€‚

3. **è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•**ï¼šNucleotide Transformerä½¿ç”¨äº†ç±»ä¼¼äºBERTçš„æ©ç è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹è¢«é®ç›–çš„æ ¸è‹·é…¸æ¥å­¦ä¹ DNAçš„é€šç”¨è¡¨ç¤ºã€‚è¿™ç§é¢„è®­ç»ƒæ–¹å¼ä¸éœ€è¦ä»»ä½•ç‰¹å®šåŠŸèƒ½çš„æ ‡æ³¨æ•°æ®ã€‚

4. **è¿ç§»å­¦ä¹ æ½œåŠ›**ï¼šè¿™ç§é€šç”¨è¡¨ç¤ºå¯ä»¥è¢«"è¿ç§»"åˆ°å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚å¯åŠ¨å­é¢„æµ‹ã€å¢å¼ºå­é¢„æµ‹ã€å‰ªæ¥ä½ç‚¹è¯†åˆ«ç­‰ï¼Œåªéœ€å°‘é‡çš„å¾®è°ƒæˆ–ç®€å•çš„æ¢æµ‹ã€‚

ç®€å•ç±»æ¯”ï¼šè¿™å°±åƒä¸€ä¸ªäººé€šè¿‡å¤§é‡é˜…è¯»å„ç±»æ–‡ç« å­¦ä¹ äº†è¯­è¨€çš„ä¸€èˆ¬è§„å¾‹ï¼Œç„¶åå¯ä»¥å°†è¿™ç§ç†è§£åº”ç”¨åˆ°å†™ä½œã€ç¿»è¯‘ã€æ–‡æ³•åˆ†æç­‰ç‰¹å®šä»»åŠ¡ä¸Šï¼Œè€Œä¸éœ€è¦ä¸ºæ¯é¡¹ä»»åŠ¡ä»å¤´å­¦ä¹ è¯­è¨€ã€‚
```
  
- Models acquire knowledge about genomic elements and can predict variant effects
- Demonstrates that training on diverse species outperforms training on only human sequences

### 2. Methodological Framework
- Collection of models with varying parameters (50M to 2.5B) trained on different datasets:
  1. Human reference genome (500M parameters)
  2. 1000 Genomes Project - 3,202 diverse human genomes (500M and 2.5B parameters)
  3. Multispecies dataset - 850 diverse species (2.5B parameters)
- Two evaluation strategies:
  1. Probing: using embeddings as features for downstream models
  2. Parameter-efficient fine-tuning: adapting models for specific tasks

### 3. Validation Strategy
- Benchmarked on 18 diverse genomic prediction tasks:
  - Histone modifications
  - Enhancer/promoter prediction
  - Splice site prediction
- Compared against supervised baselines (BPNet) and other foundation models (DNABERT-2, HyenaDNA, Enformer)
- Additional evaluation on specialized tasks (DeepSEA, SpliceAI, DeepSTARR)
- Analysis of model attention to understand learned genomic patterns
- Zero-shot and fine-tuned prediction of variant effects

## ğŸ”¬ Critical Technical Details

![Overview of NT training](../../../paper-figures/Nucleotide_Transformer.png)

### 1. Model Architecture and Training
- Encoder-only transformer architecture
- 6-mer tokenization (vocabulary of 4,104 tokens)
```
**"6-mer tokenization (vocabulary of 4,104 tokens)"**çš„è¯¦ç»†è§£é‡Šï¼š

1. **ä»€ä¹ˆæ˜¯6-mer**ï¼š
   - DNAåºåˆ—ç”±Aã€Tã€Gã€Cå››ç§æ ¸è‹·é…¸ç»„æˆ
   - "6-mer"è¡¨ç¤ºå°†DNAåºåˆ—åˆ‡åˆ†ä¸ºé•¿åº¦ä¸º6çš„è¿ç»­ç‰‡æ®µ
   - ä¾‹å¦‚åºåˆ—"ATGGCAATG"å¯ä»¥åˆ‡åˆ†ä¸º"ATGGCA"ã€"TGGCAA"ã€"GGCAAT"ã€"GCAATG"è¿™æ ·çš„6-mer

2. **è¯æ±‡è¡¨å¤§å°è®¡ç®—**ï¼š
   - ç†è®ºä¸Šï¼Œ6-merçš„å¯èƒ½ç»„åˆæœ‰4^6 = 4,096ç§ï¼ˆå› ä¸ºæ¯ä¸ªä½ç½®æœ‰4ç§å¯èƒ½çš„æ ¸è‹·é…¸ï¼‰
   - 4,104 = 4,096 + 8ï¼Œå…¶ä¸­é¢å¤–çš„8ä¸ªtokenåŒ…æ‹¬ï¼š
     - 4ä¸ªå•ç‹¬çš„æ ¸è‹·é…¸è¡¨ç¤º(A, T, G, C)
     - Nè¡¨ç¤ºï¼ˆç”¨äºè¡¨ç¤ºä¸ç¡®å®šçš„æ ¸è‹·é…¸ï¼‰
     - ç‰¹æ®Šæ ‡è®°å¦‚PADï¼ˆå¡«å……ï¼‰ã€MASKï¼ˆæ©ç ï¼‰å’ŒCLSï¼ˆåºåˆ—å¼€å§‹ï¼‰æ ‡è®°
è¿™å‡ ä¸ªç‰¹æ®Šæ ‡è®°(PAD, MASK, CLS)æºè‡ªäºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„BERTæ¨¡å‹ï¼Œåœ¨Nucleotide Transformerä¸­è¢«åº”ç”¨äºDNAåºåˆ—å¤„ç†ï¼š

### PADï¼ˆå¡«å……ï¼‰æ ‡è®°
- **ç›®çš„**ï¼šä½¿æ‰¹å¤„ç†ä¸­çš„æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
- **å·¥ä½œåŸç†**ï¼šå½“DNAåºåˆ—é•¿åº¦ä¸è¶³æ‰€éœ€çš„é•¿åº¦æ—¶ï¼ˆå¦‚ä¸è¶³1000ä¸ªtokenï¼‰ï¼Œç”¨PADæ ‡è®°å¡«å……
- **ç‰¹ç‚¹**ï¼šæ¨¡å‹è¢«è®­ç»ƒä¸ºå¿½ç•¥è¿™äº›æ ‡è®°çš„å†…å®¹ï¼Œä¸å°†å®ƒä»¬çº³å…¥æŸå¤±å‡½æ•°è®¡ç®—
- **å®é™…åº”ç”¨**ï¼šå…è®¸GPUé«˜æ•ˆå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—

### MASKï¼ˆæ©ç ï¼‰æ ‡è®°
- **ç›®çš„**ï¼šå®ç°æ©ç è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒ
- **å·¥ä½œåŸç†**ï¼šéšæœºé®ç›–è¾“å…¥åºåˆ—ä¸­çº¦15%çš„tokenï¼Œæ›¿æ¢ä¸ºMASKæ ‡è®°
- **è®­ç»ƒä»»åŠ¡**ï¼šæ¨¡å‹éœ€è¦é¢„æµ‹è¿™äº›è¢«é®ç›–ä½ç½®çš„åŸå§‹æ ¸è‹·é…¸åºåˆ—
- **ä½œç”¨**ï¼šå¼ºåˆ¶æ¨¡å‹å­¦ä¹ DNAåºåˆ—çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»å’Œç”Ÿç‰©å­¦æ¨¡å¼

### CLSï¼ˆåºåˆ—å¼€å§‹ï¼‰æ ‡è®°
- **ç›®çš„**ï¼šæ ‡è®°åºåˆ—çš„å¼€å§‹å¹¶æä¾›æ•´ä¸ªåºåˆ—çš„è¡¨ç¤º
- **å·¥ä½œåŸç†**ï¼šæ¯ä¸ªè¾“å…¥åºåˆ—çš„æœ€å‰é¢æ·»åŠ CLSæ ‡è®°
- **ç‰¹ç‚¹**ï¼šä¸è¯¥æ ‡è®°å…³è”çš„åµŒå…¥å¯ä»¥æ•è·æ•´ä¸ªåºåˆ—çš„å…¨å±€ä¿¡æ¯
- **åº”ç”¨**ï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒCLSæ ‡è®°çš„æœ€ç»ˆåµŒå…¥å¸¸ç”¨äºé¢„æµ‹æ•´ä¸ªåºåˆ—çš„å±æ€§

è¿™äº›ç‰¹æ®Šæ ‡è®°ä½¿æ¨¡å‹èƒ½å¤Ÿï¼š
1. é«˜æ•ˆå¤„ç†å˜é•¿åºåˆ—
2. å­¦ä¹ DNAåºåˆ—çš„å†…åœ¨ç»“æ„å’ŒåŠŸèƒ½
3. ç”Ÿæˆæ—¢åŒ…å«å±€éƒ¨åˆåŒ…å«å…¨å±€ä¿¡æ¯çš„åºåˆ—è¡¨ç¤º

åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œè¿™äº›æœºåˆ¶å¸®åŠ©æ¨¡å‹å»ºç«‹å¯¹DNAè¯­è¨€çš„åŸºæœ¬ç†è§£ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›è‰¯å¥½çš„åˆå§‹åŒ–ã€‚

3. **ä¸ºä»€ä¹ˆä½¿ç”¨6-mer**ï¼š
   - 6-meræ˜¯åºåˆ—é•¿åº¦ï¼ˆæœ€å¤šå¯å¤„ç†6kbï¼‰å’ŒåµŒå…¥å¤§å°ä¹‹é—´çš„æŠ˜ä¸­æ–¹æ¡ˆ
   - ç ”ç©¶è€…å‘ç°6-meråœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–tokené•¿åº¦
   - ä½¿ç”¨6-merå¯ä»¥æ•è·çŸ­è·ç¦»çš„æ ¸è‹·é…¸ä¾èµ–å…³ç³»

4. **tokenizationè¿‡ç¨‹**ï¼š
   - åºåˆ—é¦–å…ˆæ·»åŠ CLSï¼ˆclassï¼‰æ ‡è®°
   - ç„¶åä»å·¦è‡³å³ï¼Œå°½å¯èƒ½åŒ¹é…6-meræ ‡è®°
   - å½“æ— æ³•åŒ¹é…ï¼ˆå¦‚åºåˆ—ä¸­å«Næˆ–é•¿åº¦ä¸æ˜¯6çš„å€æ•°ï¼‰æ—¶ï¼Œä½¿ç”¨å•æ ¸è‹·é…¸æ ‡è®°

è¿™ç§åˆ†è¯æ–¹å¼å…è®¸æ¨¡å‹å¤„ç†é•¿è¾¾6kbï¼ˆv1æ¨¡å‹ï¼‰æˆ–12kbï¼ˆv2æ¨¡å‹ï¼‰çš„DNAåºåˆ—ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡å’Œæœ‰æ•ˆæ•è·åºåˆ—æ¨¡å¼çš„èƒ½åŠ›ã€‚
```
- Context length: 6kb for v1 models, 12kb for v2 models
- Pre-training using masked language modeling approach (BERT-style)
- Training infrastructure: 128 A100 GPUs across 16 nodes
- NT-v2 models incorporate architectural improvements:
  - Rotary embeddings instead of learned positional embeddings
  - SwiGLU activations
  - Removal of MLP biases and dropout mechanisms
```
NT-v2æ¨¡å‹å¼•å…¥äº†å‡ é¡¹æ¶æ„æ”¹è¿›ï¼Œè¿™äº›æ”¹è¿›åŸºäºæœ€æ–°çš„è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶å‘ç°ï¼Œç›®çš„æ˜¯æé«˜æ¨¡å‹æ€§èƒ½å¹¶å¢å¼ºè®­ç»ƒæ•ˆç‡:

### 1. Rotary embeddings æ›¿ä»£ learned positional embeddings
- **ä¼ ç»Ÿä½ç½®ç¼–ç **ï¼šv1æ¨¡å‹ä½¿ç”¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Œä¸ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„å‘é‡è¡¨ç¤º
- **æ—‹è½¬ä½ç½®ç¼–ç (RoPE)**ï¼šä¸€ç§æ•°å­¦ä¸Šæ›´ä¼˜é›…çš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œé€šè¿‡å¤æ•°æ—‹è½¬æ“ä½œå°†ç›¸å¯¹ä½ç½®ä¿¡æ¯ç¼–ç å…¥tokenè¡¨ç¤ºä¸­
- **ä¼˜åŠ¿**ï¼š
  - æ›´å¥½åœ°å¤„ç†åºåˆ—ä¸­çš„ç›¸å¯¹ä½ç½®å…³ç³»
  - æ›´å®¹æ˜“æ‰©å±•åˆ°æ›´é•¿çš„åºåˆ—
  - åœ¨æ¯ä¸ªæ³¨æ„åŠ›å±‚åº”ç”¨ï¼Œè€Œä¸ä»…æ˜¯åœ¨è¾“å…¥å±‚
  - æé«˜äº†æ¨¡å‹å¯¹ä½ç½®ä¿¡æ¯çš„ç¼–ç æ•ˆç‡

å¥½çš„ï¼Œæˆ‘ä»¬**ä¸€æ­¥æ­¥æ‹†è§£** Rotary Embeddingsï¼ˆRoPEï¼‰çš„æ¦‚å¿µï¼Œå¹¶ç”¨ç›´è§‚çš„**å›¾ç‰‡ç±»æ¯”**æ¥å¸®åŠ©ç†è§£ã€‚

---

## **1. ä»€ä¹ˆæ˜¯â€œä½ç½®ç¼–ç â€ï¼Ÿï¼ˆPositional Embeddingsï¼‰**
åœ¨ç¥ç»ç½‘ç»œï¼ˆå¦‚Transformerï¼‰ä¸­ï¼Œæ–‡æœ¬åºåˆ—ä¸­çš„**æ¯ä¸ªå•è¯ï¼ˆTokenï¼‰** éƒ½éœ€è¦è½¬æ¢æˆä¸€ä¸ªæ•°å€¼å‘é‡æ¥è¾“å…¥æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œ**è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰** æœ¬èº«ä¸ç†è§£å•è¯åœ¨å¥å­ä¸­çš„é¡ºåºï¼Œå› æ­¤éœ€è¦ç»™æ¯ä¸ª Token **æ·»åŠ ä½ç½®ä¿¡æ¯**ï¼Œè¿™æ ·æ¨¡å‹æ‰èƒ½æ­£ç¡®ç†è§£å¥å­ç»“æ„ã€‚

### **ç±»æ¯”**
ğŸ”¹ **æƒ³è±¡ä¸€ä¸ªç®¡å¼¦ä¹é˜Ÿ**ï¼Œæ‰€æœ‰ä¹å™¨ï¼ˆTokenï¼‰ä¸€èµ·æ¼”å¥ï¼Œä½†å¦‚æœæ²¡æœ‰æŒ‡æŒ¥ï¼ˆPositional Embeddingï¼‰å‘Šè¯‰å®ƒä»¬**åœ¨ä½•æ—¶æ¼”å¥**ï¼ŒéŸ³ä¹å°±ä¼šå˜å¾—æ··ä¹±ã€‚å› æ­¤ï¼Œä½ç½®ç¼–ç å°±æ˜¯**è®©æ¨¡å‹çŸ¥é“æ¯ä¸ª Token åœ¨å¥å­ä¸­çš„â€œæ—¶é—´ä½ç½®â€**ã€‚

---

## **2. ä¼ ç»Ÿä½ç½®ç¼–ç ï¼ˆLearned Positional Embeddingsï¼‰**
ğŸ”¹ **æ–¹æ³•**ï¼š
- æ¯ä¸ª Token ä½ç½® \( i \) éƒ½æœ‰ä¸€ä¸ª**å”¯ä¸€çš„å¯å­¦ä¹ å‘é‡**ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´è¿™äº›å‘é‡ï¼Œä»¥ä¾¿æ‰¾åˆ°æœ€ä¼˜çš„è¡¨ç¤ºæ–¹å¼ã€‚
- è¿™äº›å‘é‡æ˜¯**å›ºå®šçš„**ï¼Œä¸€æ—¦è®­ç»ƒå¥½ï¼Œå°±æ— æ³•éšæ„æ‰©å±•åˆ°æ›´é•¿çš„åºåˆ—ã€‚

### **ç¤ºæ„å›¾**
Imagine æ¯ä¸ª Tokenï¼ˆå•è¯ï¼‰æœ‰ä¸€ä¸ªå”¯ä¸€çš„ä½ç½®æ ‡è®°ï¼š

Token1 â†’ ä½ç½®ç¼–ç : [0.1, 0.2, 0.3, ...]
Token2 â†’ ä½ç½®ç¼–ç : [0.5, 0.4, 0.6, ...]
Token3 â†’ ä½ç½®ç¼–ç : [0.7, 0.9, 0.1, ...]

ä½†å¦‚æœå¥å­é•¿åº¦æ¯”è®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦**æ›´é•¿**ï¼Œæ¨¡å‹å°±æ— æ³•æ­£ç¡®ç¼–ç æ–°çš„ä½ç½®ä¿¡æ¯ã€‚

### **é—®é¢˜**
âœ… å®¹æ˜“è®­ç»ƒ  
âŒ ä¸èƒ½æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—  
âŒ åªç¼–ç **ç»å¯¹ä½ç½®**ï¼ˆæ— æ³•å¾ˆå¥½è¡¨ç¤º**ç›¸å¯¹ä½ç½®**ï¼‰  

---

## **3. æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼ŒRotary Positional Embeddingsï¼‰**
ğŸ”¹ **æ–¹æ³•**ï¼š
- é€šè¿‡**æ—‹è½¬å‘é‡**ï¼ˆRotations in complex spaceï¼‰æ¥ç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä¸ºæ¯ä¸ªä½ç½®åˆ›å»ºå›ºå®šçš„ç¼–ç ã€‚
- RoPE é€šè¿‡ **ä½™å¼¦ï¼ˆcosï¼‰å’Œæ­£å¼¦ï¼ˆsinï¼‰å‡½æ•°** è¿›è¡Œæ—‹è½¬ï¼Œä½¿å¾—**ç›¸å¯¹ä½ç½®ä¿¡æ¯**è‡ªç„¶åœ°å½±å“ Token ä¹‹é—´çš„äº¤äº’ï¼Œè€Œä¸æ˜¯åªä¾èµ–ç»å¯¹ä½ç½®ã€‚

### **ç¤ºæ„å›¾**
Imagine **æ¯ä¸ª Token æ˜¯ä¸€ä¸ªé’Ÿè¡¨æŒ‡é’ˆ**ï¼ŒæŒ‡é’ˆçš„è§’åº¦ï¼ˆæ—‹è½¬æ–¹å‘ï¼‰ä»£è¡¨å®ƒçš„ä½ç½®ï¼š

Token1 â†’ æ—‹è½¬è§’åº¦: 10Â°
Token2 â†’ æ—‹è½¬è§’åº¦: 20Â°
Token3 â†’ æ—‹è½¬è§’åº¦: 30Â°

æ‰€æœ‰ Token çš„è§’åº¦**ä¸€èµ·æ—‹è½¬**ï¼Œè¿™æ ·å®ƒä»¬çš„**ç›¸å¯¹è§’åº¦**å§‹ç»ˆä¿æŒä¸€è‡´ï¼Œæ„å‘³ç€æ¨¡å‹èƒ½**è‡ªåŠ¨æ•æ‰ Token ä¹‹é—´çš„ç›¸å¯¹ä½ç½®**ã€‚

### **æ•°å­¦è¡¨ç¤º**
- ä¼ ç»Ÿç¼–ç ï¼š\( Pos(i) = Embedding(i) \)
- RoPEï¼š\( Token_i = e^{i\theta} \times Token_i \)

å…¶ä¸­ \( e^{i\theta} \) æ˜¯**å¤æ•°æ—‹è½¬**ï¼Œä½¿å¾—æ¯ä¸ª Token é€šè¿‡**è§’åº¦åç§»**æ¥ç¼–ç å®ƒçš„ä½ç½®ã€‚

---

## **4. ä¸ºä»€ä¹ˆ RoPE æ›´å¥½ï¼Ÿ**
| **æ–¹æ³•** | **ç‰¹ç‚¹** | **é—®é¢˜** |
|---|---|---|
| ä¼ ç»Ÿä½ç½®ç¼–ç  | è®­ç»ƒæ—¶å­¦ä¹ å›ºå®šç¼–ç ï¼Œæ— æ³•å¤–æ¨ | ä¸èƒ½æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ— |
| RoPE | é€šè¿‡æ—‹è½¬è§’åº¦ç¼–ç ç›¸å¯¹ä½ç½® | è®¡ç®—æ›´é«˜æ•ˆï¼Œé€‚ç”¨äºé•¿æ–‡æœ¬ |

âœ… **ç›¸å¯¹ä½ç½®ä¿¡æ¯**ï¼šRoPE ç›´æ¥ç¼–ç ç›¸å¯¹ä½ç½®ï¼Œè€Œä¸æ˜¯ä¾èµ–ç»å¯¹ä½ç½®ã€‚  
âœ… **æ›´é•¿çš„åºåˆ—**ï¼šRoPE çš„æ—‹è½¬ç¼–ç å¯ä»¥æ— é™æ‰©å±•ï¼Œä¸åƒä¼ ç»Ÿæ–¹æ³•æœ‰é•¿åº¦é™åˆ¶ã€‚  
âœ… **æ¯ä¸€å±‚éƒ½èƒ½ä½¿ç”¨**ï¼šRoPE åœ¨**æ¯ä¸ªæ³¨æ„åŠ›å±‚** éƒ½å¯ä»¥åº”ç”¨ï¼Œè€Œä¸æ˜¯åªåœ¨è¾“å…¥å±‚ã€‚

---

### **æ€»ç»“**
âœ… RoPE ä¸æ˜¯ç›´æ¥ç»™æ¯ä¸ª Token ä¸€ä¸ªç‹¬ç«‹çš„ç¼–ç ï¼Œè€Œæ˜¯**é€šè¿‡æ—‹è½¬è§’åº¦**æ¥è¡¨ç¤ºä½ç½®ã€‚  
âœ… è¿™æ ·ï¼Œå³ä½¿è¾“å…¥åºåˆ—å˜é•¿ï¼Œæ¨¡å‹ä»ç„¶å¯ä»¥æ­£ç¡®ç†è§£ **ç›¸å¯¹ä½ç½®å…³ç³»**ã€‚  
âœ… RoPE å¯ä»¥åœ¨**æ¯ä¸€å±‚æ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨**ï¼Œè€Œä¸ä»…é™äºè¾“å…¥å±‚ï¼Œæé«˜æ¨¡å‹çš„**è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›**ã€‚

### 2. SwiGLU activations
- **ä¼ ç»Ÿæ¿€æ´»å‡½æ•°**ï¼šv1æ¨¡å‹ä½¿ç”¨GELU(Gaussian Error Linear Unit)æ¿€æ´»å‡½æ•°
- **SwiGLU**ï¼šç»“åˆäº†Swishæ¿€æ´»å‡½æ•°å’Œé—¨æ§çº¿æ€§å•å…ƒ(GLU)çš„ç‰¹æ€§
- **å·¥ä½œåŸç†**ï¼šSwiGLU(x) = Swish(xW) âŠ™ (xV)ï¼Œå…¶ä¸­âŠ™æ˜¯å…ƒç´ é—´ä¹˜æ³•
- **ä¼˜åŠ¿**ï¼š
  - æ›´å¹³æ»‘çš„æ¢¯åº¦æµ
  - æ›´æœ‰æ•ˆçš„ä¿¡æ¯æµåŠ¨
  - é€šå¸¸èƒ½åŠ å¿«è®­ç»ƒé€Ÿåº¦å¹¶æé«˜æ¨¡å‹æ€§èƒ½

çœ‹èµ·æ¥ç¯å¢ƒå†…å­˜ä¸è¶³å¯¼è‡´æ— æ³•åŠ è½½ PyTorchï¼Œæˆ‘å¯ä»¥ç”¨ **NumPy** æ¥é‡æ–°ç»˜åˆ¶ **GELUã€Swish å’Œ SwiGLU æ¿€æ´»å‡½æ•°**ï¼Œè®©ä½ ç›´è§‚ç†è§£å®ƒä»¬çš„åŒºåˆ«ã€‚è®©æˆ‘é‡æ–°ç”Ÿæˆå¯è§†åŒ–å›¾ï¼

çœ‹æ¥ç¯å¢ƒä»ç„¶å­˜åœ¨é—®é¢˜ï¼Œæ— æ³•æ‰§è¡Œä»£ç ã€‚æˆ‘å¯ä»¥è§£é‡Š SwiGLU çš„æ¦‚å¿µï¼Œå¹¶æè¿°åº”è¯¥å¦‚ä½•å¯è§†åŒ–å®ƒï¼š

---

### **1. ä¼ ç»Ÿæ¿€æ´»å‡½æ•°ï¼ˆGELUï¼‰**
- GELUï¼ˆGaussian Error Linear Unitï¼‰ æ˜¯ Transformer é‡Œå¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œå®ƒæ˜¯ ReLU çš„æ”¹è¿›ç‰ˆæœ¬ï¼š
  \[
  GELU(x) = 0.5x(1 + \tanh(\sqrt{2/\pi} (x + 0.044715x^3)))
  \]
- **ç‰¹ç‚¹**ï¼š
  - å¹³æ»‘è¿‡æ¸¡ï¼Œä¸åƒ ReLU é‚£æ ·ç›´æ¥å‰ªè£è´Ÿå€¼ã€‚
  - é€‚ç”¨äºæ·±åº¦å­¦ä¹ ï¼Œæ¢¯åº¦ç¨³å®šã€‚

---

### **2. Swish æ¿€æ´»å‡½æ•°**
- Swish ç”± Google æå‡ºï¼Œå®ƒçš„å…¬å¼æ˜¯ï¼š
  \[
  Swish(x) = x \cdot \sigma(x)
  \]
  å…¶ä¸­ï¼Œ\(\sigma(x)\) æ˜¯ Sigmoid å‡½æ•°ã€‚

---

### **3. SwiGLUï¼ˆSwish + Gated Linear Unitï¼‰**
- SwiGLU ç»“åˆäº† **Swish** å’Œ **é—¨æ§çº¿æ€§å•å…ƒï¼ˆGLUï¼‰**ï¼Œè®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š
  \[
  SwiGLU(x) = Swish(xW) \odot (xV)
  \]
  å…¶ä¸­ï¼š
  - \(W\) å’Œ \(V\) æ˜¯ä¸¤ä¸ªä¸åŒçš„æƒé‡çŸ©é˜µã€‚
  - \(\odot\) ä»£è¡¨é€å…ƒç´ ç›¸ä¹˜ã€‚

---

### **4. ä¸ºä»€ä¹ˆ SwiGLU æ›´å¥½ï¼Ÿ**
| **æ–¹æ³•** | **ç‰¹ç‚¹** | **ä¼˜åŠ¿** |
|---|---|---|
| GELU | å¹³æ»‘éçº¿æ€§æ¿€æ´» | è®­ç»ƒç¨³å®šï¼Œä½†å¯èƒ½ä¿¡æ¯æµä¸å¤Ÿå¼º |
| Swish | å¹³æ»‘è‡ªé€‚åº”æ¿€æ´» | ä¿¡æ¯æµæ›´å¥½ï¼Œæ¢¯åº¦ç¨³å®š |
| SwiGLU | ç»“åˆ Swish å’Œé—¨æ§ | **è®­ç»ƒæ›´å¿«ã€æ€§èƒ½æ›´å¼ºã€è®¡ç®—æ•ˆç‡é«˜** |

âœ… SwiGLU **æé«˜ä¿¡æ¯æµåŠ¨**ï¼Œæ¯” GELU **æ›´é«˜æ•ˆ**ï¼Œæ¯” Swish **æ›´å…·çµæ´»æ€§**ã€‚

---

### **5. è§†è§‰ç±»æ¯”**
å¦‚æœä½ æƒ³è±¡ **æ¿€æ´»å‡½æ•°åƒæ°´ç®¡æ§åˆ¶æ°´æµ**ï¼š
- GELUï¼šæ™®é€šæ°´é¾™å¤´ï¼Œæµé‡è¾ƒç¨³å®šã€‚
- Swishï¼šå¯ä»¥è°ƒèŠ‚æµé‡çš„æ°´é¾™å¤´ï¼Œä½¿æ°´æ›´å¹³ç¨³æµåŠ¨ã€‚
- SwiGLUï¼šå¸¦æœ‰**æ™ºèƒ½å¼€å…³**çš„æ°´é¾™å¤´ï¼Œåªè®©é‡è¦çš„ä¿¡æ¯æµè¿‡ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚

---

### 3. ç§»é™¤MLPåç½®å’Œdropoutæœºåˆ¶
- **MLPåç½®(bias)**ï¼šä¼ ç»Ÿå‰é¦ˆç½‘ç»œä¸­çš„å¯å­¦ä¹ åç½®é¡¹
- **Dropout**ï¼šè®­ç»ƒä¸­éšæœºå…³é—­éƒ¨åˆ†ç¥ç»å…ƒçš„æ­£åˆ™åŒ–æŠ€æœ¯
- **ä¸ºä»€ä¹ˆç§»é™¤**ï¼š
  - ç ”ç©¶å‘ç°åœ¨å¤§å‹Transformeræ¨¡å‹ä¸­ï¼Œç§»é™¤è¿™äº›ç»„ä»¶å¯ä»¥ç®€åŒ–æ¨¡å‹
  - å‡å°‘è¿‡æ‹Ÿåˆé£é™©
  - ä½¿æ¨¡å‹è¡Œä¸ºæ›´åŠ å¯é¢„æµ‹
  - åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒè®¾ç½®ä¸­é€šå¸¸ä¸ä¼šæŸå¤±æ€§èƒ½

è¿™äº›æ¶æ„æ”¹è¿›å…±åŒå¸¦æ¥çš„æ•ˆæœï¼š
- NT-v2 50Mæ¨¡å‹æ€§èƒ½å¯ä¸NT-v1 500Mæ¨¡å‹ç›¸å½“
- å…è®¸å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡(ä»6kbæ‰©å±•åˆ°12kb)
- è®­ç»ƒæ•ˆç‡æé«˜
- æ¨ç†é€Ÿåº¦æ›´å¿«
- å‚æ•°åˆ©ç”¨æ•ˆç‡æ›´é«˜

è¿™äº›æ”¹è¿›åæ˜ äº†æ·±åº¦å­¦ä¹ æ¶æ„çš„æœ€æ–°å‘å±•è¶‹åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢†åŸŸçš„è¿›å±•ã€‚
```

### 2. Parameter-Efficient Fine-Tuning
- IA3 technique requiring only 0.1% of total parameters
- Introduces three learned vectors per transformer layer
- Allows fine-tuning on a single GPU in under 15 minutes
- Comparable performance to full model fine-tuning

### 3. Key Results Quantification
- NT-v2 250M model achieved best performance (MCC 0.769) across benchmark tasks
- Multispecies 2.5B model matched or exceeded BPNet baselines on 14/18 tasks
- Zero-shot variant effect prediction: AUC 0.7-0.8 for functional variants
- NT-v2 500M-parameter model with 12kb context matched/outperformed SpliceAI-10k
- NT-v2 models reduced parameters by 50x while maintaining/improving performance

## ğŸ§ª Baseline Models, Evaluation Metrics, and Datasets

### Baseline Models
- BPNet: State-of-the-art convolutional architecture (original 121K and large 28M parameters)
- DNABERT-2: Pre-trained transformer for DNA
- HyenaDNA: Models with 1kb and 32kb context lengths
- Enformer: Architecture for long-range genomic interactions

### Evaluation Metrics
- Matthews Correlation Coefficient (MCC) for classification tasks
- Area Under Curve (AUC) for chromatin profiles
- Precision-Recall AUC and top-k accuracy for splice prediction
- Pearson correlation for enhancer activity prediction

### Datasets
- Pre-training:
  - Human reference genome: 3.2B nucleotides
  - 1000G: 3,202 human genomes (20.5T nucleotides)
  - Multispecies: 850 genomes across diverse phyla (174B nucleotides)
- Benchmark: 18 curated genomic datasets from:
  - ENCODE (histone marks, enhancers)
  - GENCODE (splice sites)
  - Eukaryotic Promoter Database
- Variant effect datasets:
  - eQTLs and meQTLs from GRASP
  - Pathogenic variants from ClinVar and HGMD

## ğŸ’­ Critical Research Implications

### 1. Methodological Impact
- Demonstrates viability of self-supervised learning for genomics
- Provides robust benchmark for evaluating genomic foundation models
- Shows scaling laws in genomics modeling similar to NLP
- Establishes parameter-efficient fine-tuning as effective approach
- Challenges need for supervised pre-training

### 2. Biological Insights
- Models learn biologically meaningful features without supervision
- Attention maps reveal focus on functional elements
- Framework for understanding variant effects beyond protein-coding regions
- Suggests multispecies training captures conserved regulatory elements

### 3. Practical Applications
- Efficient prediction of molecular phenotypes from DNA
- Variant prioritization for clinical interpretation
- Characterization of non-coding regulatory regions
- Cross-species genomic annotation transfer
- Model compression while maintaining performance

## ğŸ“Š Future Research Directions

### 1. Technical Extensions
- Extending context length beyond 12kb for capturing long-range interactions
- Specialized architectures for specific genomic tasks
- Integration with other omics data modalities
- Further architectural optimizations based on demonstrated scaling laws
- Exploration of different tokenization strategies

### 2. Biological Questions
- Characterization of novel regulatory elements
- Understanding species-specific vs. conserved genomic patterns
- Mechanistic interpretation of attention patterns
- Exploration of epigenetic marks and 3D chromatin structure

### 3. Application Areas
- Personalized genomic interpretation
- Disease risk prediction from genetic variants
- Cross-species comparative genomics
- Regulatory element discovery
- Synthetic genomics and genome engineering

## ğŸ’¡ Implementation Notes

### 1. Key Requirements
- High-performance computing resources for larger models
- Jax/PyTorch environment for model training/inference
- Efficient data pipelines for processing genomic sequences
- Parameter-efficient fine-tuning implementation
- Evaluation framework for diverse downstream tasks

### 2. Critical Considerations
- Choice of tokenization strategy (6-mers vs alternatives)
- Balancing model size vs. performance requirements
- Managing genomic data heterogeneity
- Task-specific fine-tuning strategies
- Appropriately splitting genomic data to avoid contamination
- Interpreting model outputs in biologically meaningful ways
