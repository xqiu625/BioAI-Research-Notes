## ğŸ“Š Paper Metadata
- **Title**: Nucleotide Transformer: building and evaluating robust foundation models for human genomics
- **Authors**: Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo P. de Almeida, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir, Marie Lopez, Thomas Pierrot
- **Publication**: Nature Methods
- **Institution**: InstaDeep, NVIDIA, Technical University of Munich
- **Paper Link**: https://doi.org/10.1038/s41592-024-02523-z
- **Code/Data**: https://github.com/instadeepai/nucleotide-transformer
- **Date Read**: 2025-02-18

## ğŸ”„ Key Scientific Insights

### 1. Conceptual Innovation
- Development of transformer-based foundation models (Nucleotide Transformer) pre-trained on DNA sequences
- Models can learn context-specific representations of nucleotide sequences without task-specific supervision
```
è¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¸“é—¨è®­ç»ƒï¼ˆæ²¡æœ‰ä»»åŠ¡ç‰¹å®šç›‘ç£ï¼‰çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ åˆ°æ ¸è‹·é…¸åºåˆ—çš„ä¸Šä¸‹æ–‡ç‰¹å®šè¡¨ç¤ºã€‚

è¯¦ç»†è§£é‡Šï¼š

1. **æ— éœ€ä»»åŠ¡ç‰¹å®šç›‘ç£**ï¼šä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡å¸¦æ ‡ç­¾çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹å®Œæˆç‰¹å®šä»»åŠ¡ï¼ˆå¦‚é¢„æµ‹å¯åŠ¨å­ã€å¢å¼ºå­æˆ–å‰ªæ¥ä½ç‚¹ï¼‰ã€‚è¿™ç§æ–¹æ³•è¢«ç§°ä¸º"æœ‰ç›‘ç£å­¦ä¹ "ï¼Œå› ä¸ºæ¨¡å‹æ˜¯åœ¨æ˜ç¡®çŸ¥é“"æ­£ç¡®ç­”æ¡ˆ"çš„æƒ…å†µä¸‹å­¦ä¹ çš„ã€‚

2. **ä¸Šä¸‹æ–‡ç‰¹å®šè¡¨ç¤º**ï¼šè¿™äº›æ¨¡å‹èƒ½å¤Ÿç†è§£DNAåºåˆ—ä¸­æ ¸è‹·é…¸çš„ä¸Šä¸‹æ–‡å«ä¹‰ã€‚ä¾‹å¦‚ï¼ŒåŒä¸€ä¸ªæ ¸è‹·é…¸åºåˆ—åœ¨ä¸åŒåŸºå› ç¯å¢ƒä¸­å¯èƒ½æœ‰ä¸åŒçš„åŠŸèƒ½ï¼Œæ¨¡å‹å¯ä»¥æ•æ‰åˆ°è¿™ç§å·®å¼‚ã€‚è¿™äº›è¡¨ç¤ºï¼ˆä¹Ÿç§°ä¸ºåµŒå…¥æˆ–embeddingsï¼‰ç¼–ç äº†åºåˆ—çš„ç”Ÿç‰©å­¦ç‰¹æ€§å’ŒåŠŸèƒ½ä¿¡æ¯ã€‚

3. **è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•**ï¼šNucleotide Transformerä½¿ç”¨äº†ç±»ä¼¼äºBERTçš„æ©ç è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹è¢«é®ç›–çš„æ ¸è‹·é…¸æ¥å­¦ä¹ DNAçš„é€šç”¨è¡¨ç¤ºã€‚è¿™ç§é¢„è®­ç»ƒæ–¹å¼ä¸éœ€è¦ä»»ä½•ç‰¹å®šåŠŸèƒ½çš„æ ‡æ³¨æ•°æ®ã€‚

4. **è¿ç§»å­¦ä¹ æ½œåŠ›**ï¼šè¿™ç§é€šç”¨è¡¨ç¤ºå¯ä»¥è¢«"è¿ç§»"åˆ°å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚å¯åŠ¨å­é¢„æµ‹ã€å¢å¼ºå­é¢„æµ‹ã€å‰ªæ¥ä½ç‚¹è¯†åˆ«ç­‰ï¼Œåªéœ€å°‘é‡çš„å¾®è°ƒæˆ–ç®€å•çš„æ¢æµ‹ã€‚

ç®€å•ç±»æ¯”ï¼šè¿™å°±åƒä¸€ä¸ªäººé€šè¿‡å¤§é‡é˜…è¯»å„ç±»æ–‡ç« å­¦ä¹ äº†è¯­è¨€çš„ä¸€èˆ¬è§„å¾‹ï¼Œç„¶åå¯ä»¥å°†è¿™ç§ç†è§£åº”ç”¨åˆ°å†™ä½œã€ç¿»è¯‘ã€æ–‡æ³•åˆ†æç­‰ç‰¹å®šä»»åŠ¡ä¸Šï¼Œè€Œä¸éœ€è¦ä¸ºæ¯é¡¹ä»»åŠ¡ä»å¤´å­¦ä¹ è¯­è¨€ã€‚
```
  
- Models acquire knowledge about genomic elements and can predict variant effects
- Demonstrates that training on diverse species outperforms training on only human sequences

### 2. Methodological Framework
- Collection of models with varying parameters (50M to 2.5B) trained on different datasets:
  1. Human reference genome (500M parameters)
  2. 1000 Genomes Project - 3,202 diverse human genomes (500M and 2.5B parameters)
  3. Multispecies dataset - 850 diverse species (2.5B parameters)
- Two evaluation strategies:
  1. Probing: using embeddings as features for downstream models
  2. Parameter-efficient fine-tuning: adapting models for specific tasks

### 3. Validation Strategy
- Benchmarked on 18 diverse genomic prediction tasks:
  - Histone modifications
  - Enhancer/promoter prediction
  - Splice site prediction
- Compared against supervised baselines (BPNet) and other foundation models (DNABERT-2, HyenaDNA, Enformer)
- Additional evaluation on specialized tasks (DeepSEA, SpliceAI, DeepSTARR)
- Analysis of model attention to understand learned genomic patterns
- Zero-shot and fine-tuned prediction of variant effects

## ğŸ”¬ Critical Technical Details

![Overview of NT training](../../../paper-figures/Nucleotide_Transformer.png)

### 1. Model Architecture and Training
- Encoder-only transformer architecture
- 6-mer tokenization (vocabulary of 4,104 tokens)
```
**"6-mer tokenization (vocabulary of 4,104 tokens)"**çš„è¯¦ç»†è§£é‡Šï¼š

1. **ä»€ä¹ˆæ˜¯6-mer**ï¼š
   - DNAåºåˆ—ç”±Aã€Tã€Gã€Cå››ç§æ ¸è‹·é…¸ç»„æˆ
   - "6-mer"è¡¨ç¤ºå°†DNAåºåˆ—åˆ‡åˆ†ä¸ºé•¿åº¦ä¸º6çš„è¿ç»­ç‰‡æ®µ
   - ä¾‹å¦‚åºåˆ—"ATGGCAATG"å¯ä»¥åˆ‡åˆ†ä¸º"ATGGCA"ã€"TGGCAA"ã€"GGCAAT"ã€"GCAATG"è¿™æ ·çš„6-mer

2. **è¯æ±‡è¡¨å¤§å°è®¡ç®—**ï¼š
   - ç†è®ºä¸Šï¼Œ6-merçš„å¯èƒ½ç»„åˆæœ‰4^6 = 4,096ç§ï¼ˆå› ä¸ºæ¯ä¸ªä½ç½®æœ‰4ç§å¯èƒ½çš„æ ¸è‹·é…¸ï¼‰
   - 4,104 = 4,096 + 8ï¼Œå…¶ä¸­é¢å¤–çš„8ä¸ªtokenåŒ…æ‹¬ï¼š
     - 4ä¸ªå•ç‹¬çš„æ ¸è‹·é…¸è¡¨ç¤º(A, T, G, C)
     - Nè¡¨ç¤ºï¼ˆç”¨äºè¡¨ç¤ºä¸ç¡®å®šçš„æ ¸è‹·é…¸ï¼‰
     - ç‰¹æ®Šæ ‡è®°å¦‚PADï¼ˆå¡«å……ï¼‰ã€MASKï¼ˆæ©ç ï¼‰å’ŒCLSï¼ˆåºåˆ—å¼€å§‹ï¼‰æ ‡è®°
è¿™å‡ ä¸ªç‰¹æ®Šæ ‡è®°(PAD, MASK, CLS)æºè‡ªäºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„BERTæ¨¡å‹ï¼Œåœ¨Nucleotide Transformerä¸­è¢«åº”ç”¨äºDNAåºåˆ—å¤„ç†ï¼š

### PADï¼ˆå¡«å……ï¼‰æ ‡è®°
- **ç›®çš„**ï¼šä½¿æ‰¹å¤„ç†ä¸­çš„æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
- **å·¥ä½œåŸç†**ï¼šå½“DNAåºåˆ—é•¿åº¦ä¸è¶³æ‰€éœ€çš„é•¿åº¦æ—¶ï¼ˆå¦‚ä¸è¶³1000ä¸ªtokenï¼‰ï¼Œç”¨PADæ ‡è®°å¡«å……
- **ç‰¹ç‚¹**ï¼šæ¨¡å‹è¢«è®­ç»ƒä¸ºå¿½ç•¥è¿™äº›æ ‡è®°çš„å†…å®¹ï¼Œä¸å°†å®ƒä»¬çº³å…¥æŸå¤±å‡½æ•°è®¡ç®—
- **å®é™…åº”ç”¨**ï¼šå…è®¸GPUé«˜æ•ˆå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—

### MASKï¼ˆæ©ç ï¼‰æ ‡è®°
- **ç›®çš„**ï¼šå®ç°æ©ç è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒ
- **å·¥ä½œåŸç†**ï¼šéšæœºé®ç›–è¾“å…¥åºåˆ—ä¸­çº¦15%çš„tokenï¼Œæ›¿æ¢ä¸ºMASKæ ‡è®°
- **è®­ç»ƒä»»åŠ¡**ï¼šæ¨¡å‹éœ€è¦é¢„æµ‹è¿™äº›è¢«é®ç›–ä½ç½®çš„åŸå§‹æ ¸è‹·é…¸åºåˆ—
- **ä½œç”¨**ï¼šå¼ºåˆ¶æ¨¡å‹å­¦ä¹ DNAåºåˆ—çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»å’Œç”Ÿç‰©å­¦æ¨¡å¼

### CLSï¼ˆåºåˆ—å¼€å§‹ï¼‰æ ‡è®°
- **ç›®çš„**ï¼šæ ‡è®°åºåˆ—çš„å¼€å§‹å¹¶æä¾›æ•´ä¸ªåºåˆ—çš„è¡¨ç¤º
- **å·¥ä½œåŸç†**ï¼šæ¯ä¸ªè¾“å…¥åºåˆ—çš„æœ€å‰é¢æ·»åŠ CLSæ ‡è®°
- **ç‰¹ç‚¹**ï¼šä¸è¯¥æ ‡è®°å…³è”çš„åµŒå…¥å¯ä»¥æ•è·æ•´ä¸ªåºåˆ—çš„å…¨å±€ä¿¡æ¯
- **åº”ç”¨**ï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒCLSæ ‡è®°çš„æœ€ç»ˆåµŒå…¥å¸¸ç”¨äºé¢„æµ‹æ•´ä¸ªåºåˆ—çš„å±æ€§

è¿™äº›ç‰¹æ®Šæ ‡è®°ä½¿æ¨¡å‹èƒ½å¤Ÿï¼š
1. é«˜æ•ˆå¤„ç†å˜é•¿åºåˆ—
2. å­¦ä¹ DNAåºåˆ—çš„å†…åœ¨ç»“æ„å’ŒåŠŸèƒ½
3. ç”Ÿæˆæ—¢åŒ…å«å±€éƒ¨åˆåŒ…å«å…¨å±€ä¿¡æ¯çš„åºåˆ—è¡¨ç¤º

åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œè¿™äº›æœºåˆ¶å¸®åŠ©æ¨¡å‹å»ºç«‹å¯¹DNAè¯­è¨€çš„åŸºæœ¬ç†è§£ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›è‰¯å¥½çš„åˆå§‹åŒ–ã€‚

3. **ä¸ºä»€ä¹ˆä½¿ç”¨6-mer**ï¼š
   - 6-meræ˜¯åºåˆ—é•¿åº¦ï¼ˆæœ€å¤šå¯å¤„ç†6kbï¼‰å’ŒåµŒå…¥å¤§å°ä¹‹é—´çš„æŠ˜ä¸­æ–¹æ¡ˆ
   - ç ”ç©¶è€…å‘ç°6-meråœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–tokené•¿åº¦
   - ä½¿ç”¨6-merå¯ä»¥æ•è·çŸ­è·ç¦»çš„æ ¸è‹·é…¸ä¾èµ–å…³ç³»

4. **tokenizationè¿‡ç¨‹**ï¼š
   - åºåˆ—é¦–å…ˆæ·»åŠ CLSï¼ˆclassï¼‰æ ‡è®°
   - ç„¶åä»å·¦è‡³å³ï¼Œå°½å¯èƒ½åŒ¹é…6-meræ ‡è®°
   - å½“æ— æ³•åŒ¹é…ï¼ˆå¦‚åºåˆ—ä¸­å«Næˆ–é•¿åº¦ä¸æ˜¯6çš„å€æ•°ï¼‰æ—¶ï¼Œä½¿ç”¨å•æ ¸è‹·é…¸æ ‡è®°

è¿™ç§åˆ†è¯æ–¹å¼å…è®¸æ¨¡å‹å¤„ç†é•¿è¾¾6kbï¼ˆv1æ¨¡å‹ï¼‰æˆ–12kbï¼ˆv2æ¨¡å‹ï¼‰çš„DNAåºåˆ—ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡å’Œæœ‰æ•ˆæ•è·åºåˆ—æ¨¡å¼çš„èƒ½åŠ›ã€‚
```
- Context length: 6kb for v1 models, 12kb for v2 models
- Pre-training using masked language modeling approach (BERT-style)
- Training infrastructure: 128 A100 GPUs across 16 nodes
- NT-v2 models incorporate architectural improvements:
  - Rotary embeddings instead of learned positional embeddings
  - SwiGLU activations
  - Removal of MLP biases and dropout mechanisms
```
NT-v2æ¨¡å‹å¼•å…¥äº†å‡ é¡¹æ¶æ„æ”¹è¿›ï¼Œè¿™äº›æ”¹è¿›åŸºäºæœ€æ–°çš„è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶å‘ç°ï¼Œç›®çš„æ˜¯æé«˜æ¨¡å‹æ€§èƒ½å¹¶å¢å¼ºè®­ç»ƒæ•ˆç‡:

### 1. Rotary embeddings æ›¿ä»£ learned positional embeddings
- **ä¼ ç»Ÿä½ç½®ç¼–ç **ï¼šv1æ¨¡å‹ä½¿ç”¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Œä¸ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„å‘é‡è¡¨ç¤º
- **æ—‹è½¬ä½ç½®ç¼–ç (RoPE)**ï¼šä¸€ç§æ•°å­¦ä¸Šæ›´ä¼˜é›…çš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œé€šè¿‡å¤æ•°æ—‹è½¬æ“ä½œå°†ç›¸å¯¹ä½ç½®ä¿¡æ¯ç¼–ç å…¥tokenè¡¨ç¤ºä¸­
- **ä¼˜åŠ¿**ï¼š
  - æ›´å¥½åœ°å¤„ç†åºåˆ—ä¸­çš„ç›¸å¯¹ä½ç½®å…³ç³»
  - æ›´å®¹æ˜“æ‰©å±•åˆ°æ›´é•¿çš„åºåˆ—
  - åœ¨æ¯ä¸ªæ³¨æ„åŠ›å±‚åº”ç”¨ï¼Œè€Œä¸ä»…æ˜¯åœ¨è¾“å…¥å±‚
  - æé«˜äº†æ¨¡å‹å¯¹ä½ç½®ä¿¡æ¯çš„ç¼–ç æ•ˆç‡

### 2. SwiGLU activations
- **ä¼ ç»Ÿæ¿€æ´»å‡½æ•°**ï¼šv1æ¨¡å‹ä½¿ç”¨GELU(Gaussian Error Linear Unit)æ¿€æ´»å‡½æ•°
- **SwiGLU**ï¼šç»“åˆäº†Swishæ¿€æ´»å‡½æ•°å’Œé—¨æ§çº¿æ€§å•å…ƒ(GLU)çš„ç‰¹æ€§
- **å·¥ä½œåŸç†**ï¼šSwiGLU(x) = Swish(xW) âŠ™ (xV)ï¼Œå…¶ä¸­âŠ™æ˜¯å…ƒç´ é—´ä¹˜æ³•
- **ä¼˜åŠ¿**ï¼š
  - æ›´å¹³æ»‘çš„æ¢¯åº¦æµ
  - æ›´æœ‰æ•ˆçš„ä¿¡æ¯æµåŠ¨
  - é€šå¸¸èƒ½åŠ å¿«è®­ç»ƒé€Ÿåº¦å¹¶æé«˜æ¨¡å‹æ€§èƒ½

### 3. ç§»é™¤MLPåç½®å’Œdropoutæœºåˆ¶
- **MLPåç½®(bias)**ï¼šä¼ ç»Ÿå‰é¦ˆç½‘ç»œä¸­çš„å¯å­¦ä¹ åç½®é¡¹
- **Dropout**ï¼šè®­ç»ƒä¸­éšæœºå…³é—­éƒ¨åˆ†ç¥ç»å…ƒçš„æ­£åˆ™åŒ–æŠ€æœ¯
- **ä¸ºä»€ä¹ˆç§»é™¤**ï¼š
  - ç ”ç©¶å‘ç°åœ¨å¤§å‹Transformeræ¨¡å‹ä¸­ï¼Œç§»é™¤è¿™äº›ç»„ä»¶å¯ä»¥ç®€åŒ–æ¨¡å‹
  - å‡å°‘è¿‡æ‹Ÿåˆé£é™©
  - ä½¿æ¨¡å‹è¡Œä¸ºæ›´åŠ å¯é¢„æµ‹
  - åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒè®¾ç½®ä¸­é€šå¸¸ä¸ä¼šæŸå¤±æ€§èƒ½

è¿™äº›æ¶æ„æ”¹è¿›å…±åŒå¸¦æ¥çš„æ•ˆæœï¼š
- NT-v2 50Mæ¨¡å‹æ€§èƒ½å¯ä¸NT-v1 500Mæ¨¡å‹ç›¸å½“
- å…è®¸å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡(ä»6kbæ‰©å±•åˆ°12kb)
- è®­ç»ƒæ•ˆç‡æé«˜
- æ¨ç†é€Ÿåº¦æ›´å¿«
- å‚æ•°åˆ©ç”¨æ•ˆç‡æ›´é«˜

è¿™äº›æ”¹è¿›åæ˜ äº†æ·±åº¦å­¦ä¹ æ¶æ„çš„æœ€æ–°å‘å±•è¶‹åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢†åŸŸçš„è¿›å±•ã€‚
```

### 2. Parameter-Efficient Fine-Tuning
- IA3 technique requiring only 0.1% of total parameters
- Introduces three learned vectors per transformer layer
- Allows fine-tuning on a single GPU in under 15 minutes
- Comparable performance to full model fine-tuning

### 3. Key Results Quantification
- NT-v2 250M model achieved best performance (MCC 0.769) across benchmark tasks
- Multispecies 2.5B model matched or exceeded BPNet baselines on 14/18 tasks
- Zero-shot variant effect prediction: AUC 0.7-0.8 for functional variants
- NT-v2 500M-parameter model with 12kb context matched/outperformed SpliceAI-10k
- NT-v2 models reduced parameters by 50x while maintaining/improving performance

## ğŸ§ª Baseline Models, Evaluation Metrics, and Datasets

### Baseline Models
- BPNet: State-of-the-art convolutional architecture (original 121K and large 28M parameters)
- DNABERT-2: Pre-trained transformer for DNA
- HyenaDNA: Models with 1kb and 32kb context lengths
- Enformer: Architecture for long-range genomic interactions

### Evaluation Metrics
- Matthews Correlation Coefficient (MCC) for classification tasks
- Area Under Curve (AUC) for chromatin profiles
- Precision-Recall AUC and top-k accuracy for splice prediction
- Pearson correlation for enhancer activity prediction

### Datasets
- Pre-training:
  - Human reference genome: 3.2B nucleotides
  - 1000G: 3,202 human genomes (20.5T nucleotides)
  - Multispecies: 850 genomes across diverse phyla (174B nucleotides)
- Benchmark: 18 curated genomic datasets from:
  - ENCODE (histone marks, enhancers)
  - GENCODE (splice sites)
  - Eukaryotic Promoter Database
- Variant effect datasets:
  - eQTLs and meQTLs from GRASP
  - Pathogenic variants from ClinVar and HGMD

## ğŸ’­ Critical Research Implications

### 1. Methodological Impact
- Demonstrates viability of self-supervised learning for genomics
- Provides robust benchmark for evaluating genomic foundation models
- Shows scaling laws in genomics modeling similar to NLP
- Establishes parameter-efficient fine-tuning as effective approach
- Challenges need for supervised pre-training

### 2. Biological Insights
- Models learn biologically meaningful features without supervision
- Attention maps reveal focus on functional elements
- Framework for understanding variant effects beyond protein-coding regions
- Suggests multispecies training captures conserved regulatory elements

### 3. Practical Applications
- Efficient prediction of molecular phenotypes from DNA
- Variant prioritization for clinical interpretation
- Characterization of non-coding regulatory regions
- Cross-species genomic annotation transfer
- Model compression while maintaining performance

## ğŸ“Š Future Research Directions

### 1. Technical Extensions
- Extending context length beyond 12kb for capturing long-range interactions
- Specialized architectures for specific genomic tasks
- Integration with other omics data modalities
- Further architectural optimizations based on demonstrated scaling laws
- Exploration of different tokenization strategies

### 2. Biological Questions
- Characterization of novel regulatory elements
- Understanding species-specific vs. conserved genomic patterns
- Mechanistic interpretation of attention patterns
- Exploration of epigenetic marks and 3D chromatin structure

### 3. Application Areas
- Personalized genomic interpretation
- Disease risk prediction from genetic variants
- Cross-species comparative genomics
- Regulatory element discovery
- Synthetic genomics and genome engineering

## ğŸ’¡ Implementation Notes

### 1. Key Requirements
- High-performance computing resources for larger models
- Jax/PyTorch environment for model training/inference
- Efficient data pipelines for processing genomic sequences
- Parameter-efficient fine-tuning implementation
- Evaluation framework for diverse downstream tasks

### 2. Critical Considerations
- Choice of tokenization strategy (6-mers vs alternatives)
- Balancing model size vs. performance requirements
- Managing genomic data heterogeneity
- Task-specific fine-tuning strategies
- Appropriately splitting genomic data to avoid contamination
- Interpreting model outputs in biologically meaningful ways
