## ğŸ“Š Paper Metadata
- Title: Delineating the effective use of self-supervised learning in single-cell genomics
- Authors: Till Richter, Mojtaba Bahrami, Yufan Xia, David S. Fischer, Fabian J. Theis
- Publication: Nature Machine Intelligence
- Institution: Helmholtz Munich, Technical University of Munich
- Paper Link: https://doi.org/10.1038/s42256-024-00934-3
- Code/Data: https://github.com/theislab/ssl_in_scg and https://github.com/theislab/sc_mae

## ğŸ”„ Key Scientific Insights

### 1. Conceptual Innovation
- The paper investigates when and how self-supervised learning (SSL) provides advantages in single-cell genomics (SCG)
- Critical finding: SSL excels in transfer learning scenarios where insights from large auxiliary datasets can be applied to smaller or unseen datasets
- Shows that masked autoencoders outperform contrastive methods in SCG, contrary to trends in computer vision
- Demonstrates SSL's utility in zero-shot settings, cross-modality prediction, and data integration

### 2. Methodological Framework
- Uses fully connected autoencoder architectures to minimize architectural influences
- Two-stage framework: pre-training (pretext task) followed by optional fine-tuning
- Pre-training conducted on scTab dataset (>20M cells) with all 19,331 human protein-encoding genes
- Implemented multiple SSL strategies:
  1. Masked autoencoders with random masking
  2. Gene programme (GP) masking
  3. Isolated masking (GP to GP, GP to TF)
  4. Contrastive learning (BYOL, Barlow twins)

### 3. Validation Strategy
Tested across multiple downstream tasks:
- Cell-type prediction on Human Lung Cell Atlas, PBMCs, Tabula Sapiens
- Gene-expression reconstruction
- Cross-modality prediction (RNA to protein, RNA to ATAC-seq)
- Data integration across datasets with batch effects

## ğŸ”¬ Critical Technical Details

### 1. Self-Supervised Learning Implementation
- Masked autoencoders: Mask significant portions of input genes, train model to reconstruct missing parts
- Random masking: 50% of genes randomly chosen and masked
- GP masking: Sets of genes known for biological functions are masked
- Isolated masking: All genes but a defined set are masked
- Contrastive learning: Implemented BYOL and Barlow twins with negative binomial noise and masking as data augmentation

### 2. Experimental Validation Framework
- Cell-type prediction: Evaluated with macro F1 score (for class imbalance robustness) and micro F1 score
- Gene-expression reconstruction: Evaluated with weighted explained variance
- Cross-modality prediction: Pearson correlation between predicted and true values
- Data integration: scIB metrics (batch correction + biological conservation)

### 3. Key Results Quantification
- SSL pre-training on auxiliary data improved PBMC cell-type prediction from 0.7013 to 0.7466 macro F1
- Tabula Sapiens improved from 0.2722 to 0.3085 macro F1
- Zero-shot SSL achieved up to 0.6725 macro F1 score on scTab test set
- Cross-modality prediction: Improved Pearson correlation from 0.8809 to 0.8943
- Data integration: Total scIB score improved from 0.5354 to 0.5638

## Baseline Model, Evaluation Metrics, and Datasets
- Baseline models: Supervised learning, unsupervised learning, PCA, GeneFormer
- Key datasets: scTab (22.2M cells, 164 cell types), HLCA (2.28M cells, 51 cell types), PBMCs (422K cells), Tabula Sapiens (483K cells)
- Five unseen datasets for transfer learning evaluation
- NeurIPS multiomics dataset for cross-modality prediction
- Three lung datasets for data integration

## ğŸ’­ Critical Research Implications

### 1. Methodological Impact
- Clarifies when SSL is beneficial in SCG: transfer learning and unseen data scenarios
- Shows masked autoencoders outperform contrastive learning in SCG
- Demonstrates that SSL's benefits don't emerge when pre-training on the same dataset as fine-tuning
- Provides framework for effective use of SSL in biological data analysis

### 2. Practical Relevance
- Enables leveraging large datasets (like cell atlases) to improve analysis of smaller experiments
- Provides robust performance on class-imbalanced datasets, common in biological contexts
- Reduces reliance on curated labels, addressing challenges in obtaining accurate annotations
- Enhances cross-modality prediction when one modality is more abundant

### 3. Biological Insights
- Suggests different masking strategies can capture specific biological variations
- Shows importance of tailored masking strategies for capturing gene interactions
- Demonstrates potential for SSL to improve data integration while preserving biological variability
- Illustrates how cell-type classifications can be improved through transfer learning

## ğŸ“Š Future Research Directions

### 1. Technical Extensions
- Application to other architectures (e.g., transformers)
- Development of more biologically-informed masking strategies
- Extension to other modalities beyond transcriptomics
- Optimization of consensus strategies for different downstream tasks

### 2. Biological Questions
- Exploring SSL for temporal dynamics in single-cell data
- Investigating SSL for smaller, specialized datasets
- Understanding how SSL captures meaningful biological variation
- Examining cross-species transfer learning potential

### 3. Practical Applications
- Integration into cell atlas projects
- Application in clinical diagnostics with limited samples
- Development of foundation models for single-cell biology
- Improvement of multi-modal data analysis frameworks

## ğŸ’¡ Implementation Notes

### 1. Key Requirements
- GPU hardware (Tesla V100 or equivalent) with 32GB+ RAM
- 160GB system memory for full scTab dataset (at batch size 8,192)
- PyTorch implementation
- Scanpy for data preprocessing

### 2. Critical Considerations
- Memory requirements can be reduced with smaller batch sizes (but increases training time)
- Fixed random seeds needed for reproducibility
- Batch effects should be addressed with covariates when working with fewer datasets
- Appropriate evaluation metrics (macro F1 score) needed for imbalanced cell type distributions



## ğŸ“ Personal Notes

è¯¥è®ºæ–‡ç ”ç©¶äº†è‡ªç›‘ç£å­¦ä¹ (SSL)åœ¨å•ç»†èƒåŸºå› ç»„å­¦(SCG)ä¸­ä½•æ—¶ä»¥åŠå¦‚ä½•æä¾›ä¼˜åŠ¿ï¼š

* å…³é”®å‘ç°ï¼šSSLåœ¨è¿ç§»å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œä»å¤§å‹è¾…åŠ©æ•°æ®é›†è·å¾—çš„è§è§£å¯ä»¥åº”ç”¨äºè¾ƒå°æˆ–æœªè§è¿‡çš„æ•°æ®é›†
* è¡¨æ˜æ©ç è‡ªç¼–ç å™¨åœ¨SCGä¸­çš„è¡¨ç°ä¼˜äºå¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¿™ä¸è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„è¶‹åŠ¿ç›¸å
* è¯æ˜äº†SSLåœ¨é›¶æ ·æœ¬è®¾ç½®ã€è·¨æ¨¡æ€é¢„æµ‹å’Œæ•°æ®æ•´åˆæ–¹é¢çš„å®ç”¨æ€§

## è‡ªç›‘ç£å­¦ä¹ (SSL)ç›¸å…³æœ¯è¯­

* **Self-supervised learning (SSL)** - è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸€ç§ä»æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ æœ‰æ„ä¹‰è¡¨ç¤ºçš„æ–¹æ³•
* <details>
  <summary><b>Pretext task å‰ç½®ä»»åŠ¡ï¼ŒSSLä¸­ç”¨äºé¢„è®­ç»ƒçš„ä»»åŠ¡ </b></summary>
**å‰ç½®ä»»åŠ¡(Pretext task)**æ˜¯è‡ªç›‘ç£å­¦ä¹ ä¸­è®¾è®¡çš„ä¸€ç§è¾…åŠ©ä»»åŠ¡ï¼Œå…¶ç›®çš„ä¸æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªä»»åŠ¡æœ¬èº«ï¼Œè€Œæ˜¯é€šè¿‡è§£å†³è¿™ä¸ªä»»åŠ¡æ¥å­¦ä¹ æ•°æ®çš„æœ‰ç”¨è¡¨ç¤º(representation)ã€‚å‰ç½®ä»»åŠ¡çš„ç‰¹ç‚¹æ˜¯ï¼š

1. **ä¸éœ€è¦äººå·¥æ ‡æ³¨**ï¼šå‰ç½®ä»»åŠ¡å¯ä»¥ä»æ•°æ®æœ¬èº«è‡ªåŠ¨ç”Ÿæˆ"ä¼ªæ ‡ç­¾"ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚

2. **ä»»åŠ¡è®¾è®¡ç›®æ ‡**ï¼šè®¾è®¡å‰ç½®ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹åœ¨è§£å†³è¿™äº›ä»»åŠ¡çš„è¿‡ç¨‹ä¸­ï¼Œè¢«è¿«å­¦ä¹ æ•°æ®çš„åº•å±‚ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚

3. **ä¸ä¸‹æ¸¸ä»»åŠ¡çš„å…³ç³»**ï¼šå‰ç½®ä»»åŠ¡é€šå¸¸ä¸æœ€ç»ˆçš„ä¸‹æ¸¸ä»»åŠ¡(å¦‚ç»†èƒç±»å‹åˆ†ç±»)ä¸åŒï¼Œä½†è§£å†³å‰ç½®ä»»åŠ¡æ‰€å­¦åˆ°çš„ç‰¹å¾è¡¨ç¤ºå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰å¸®åŠ©ã€‚

åœ¨æœ¬è®ºæ–‡çš„å•ç»†èƒåŸºå› ç»„å­¦èƒŒæ™¯ä¸­ï¼Œå‰ç½®ä»»åŠ¡çš„ä¾‹å­åŒ…æ‹¬ï¼š
- **æ©ç é‡å»º**ï¼šéšæœºæ©ç›–ä¸€éƒ¨åˆ†åŸºå› è¡¨è¾¾å€¼ï¼Œç„¶åè®­ç»ƒæ¨¡å‹é¢„æµ‹è¿™äº›è¢«æ©ç›–çš„å€¼
- **åŸºå› ç¨‹åºæ©ç **ï¼šæ©ç›–ä¸ç‰¹å®šç”Ÿç‰©åŠŸèƒ½ç›¸å…³çš„åŸºå› é›†ï¼Œè®©æ¨¡å‹å­¦ä¹ åŸºå› é—´çš„åŠŸèƒ½å…³ç³»
- **å¯¹æ¯”å­¦ä¹ ä»»åŠ¡**ï¼šé€šè¿‡æ•°æ®å¢å¼ºåˆ›å»ºåŒä¸€ç»†èƒçš„ä¸åŒ"è§†è§’"ï¼Œè®­ç»ƒæ¨¡å‹è¯†åˆ«å“ªäº›æ ·æœ¬æ¥è‡ªåŒä¸€ä¸ªåŸå§‹æ•°æ®ç‚¹
</details>


å‰ç½®ä»»åŠ¡æ˜¯SSLçš„ç¬¬ä¸€é˜¶æ®µ(é¢„è®­ç»ƒé˜¶æ®µ)ã€‚å®Œæˆå‰ç½®ä»»åŠ¡çš„è®­ç»ƒåï¼Œæ¨¡å‹å¯ä»¥ç›´æ¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡(é›¶æ ·æœ¬è¯„ä¼°)æˆ–ç»è¿‡å¾®è°ƒåç”¨äºç‰¹å®šåº”ç”¨ã€‚
* **Pretext task** - å‰ç½®ä»»åŠ¡ï¼ŒSSLä¸­ç”¨äºé¢„è®­ç»ƒçš„ä»»åŠ¡ 
* **Zero-shot SSL** - é›¶æ ·æœ¬è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦é¢å¤–å¾®è°ƒå³å¯åº”ç”¨çš„æ¨¡å‹
* **Masked autoencoders** - æ©ç è‡ªç¼–ç å™¨ï¼Œä¸€ç§é€šè¿‡é‡å»ºè¢«æ©ç›–çš„è¾“å…¥éƒ¨åˆ†æ¥å­¦ä¹ è¡¨ç¤ºçš„æ–¹æ³•
* **Contrastive learning** - å¯¹æ¯”å­¦ä¹ ï¼Œé€šè¿‡æ¯”è¾ƒæ ·æœ¬é—´ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§å­¦ä¹ è¡¨ç¤ºçš„æ–¹æ³•
* **BYOL (Bootstrap Your Own Latent)** - ä¸€ç§æ— è´Ÿæ ·æœ¬å¯¹çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•
* **Barlow twins** - ä¸€ç§é€šè¿‡å†—ä½™åº¦å‡å°‘è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ çš„æ–¹æ³•
* **Data augmentation** - æ•°æ®å¢å¼ºï¼Œé€šè¿‡ä¿®æ”¹æ•°æ®åˆ›å»ºä¸åŒè§†è§’
* **Random masking** - éšæœºæ©ç ï¼Œéšæœºé€‰æ‹©ç‰¹å¾è¿›è¡Œæ©ç›–
* **Gene programme (GP) masking** - åŸºå› ç¨‹åºæ©ç ï¼ŒåŸºäºç”Ÿç‰©å­¦åŠŸèƒ½æ©ç›–åŸºå› é›†
