# scBERT: A Large-scale Pretrained Deep Language Model for Cell Type Annotation of Single-cell RNA-seq Data

## ğŸ“Š Paper Metadata
- **Title:** scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data
- **Authors:** [Author Names]
- **Publication:** Nature Machine Intelligence (October 2022)
- **Institution:** Tencent
- **Links:** 
  - Paper: https://www.nature.com/articles/s42256-022-00534-z
  - GitHub: https://github.com/TencentAILabHealthcare/scBERT
- **Tags:** #deep-learning #BERT #single-cell #cell-annotation #transformer

## ğŸ¯ Core Contributions
1. Novel BERT-based model adapted for single-cell RNA sequencing data analysis
2. Two-stage training paradigm with domain-specific improvements in input representation and encoder structure
3. Superior performance in cell type annotation across multiple metrics
4. Robust performance across different datasets and organs with strong batch effect resistance
5. Interpretable attention mechanism for discovering new cell-type-specific genes

## ğŸ“‹ Paper Structure
### 1. Introduction
- Importance of cell type annotation in scRNA-seq
- Limitations of current methods
- Key innovations of scBERT

### 2. Methods
- Model Architecture
  * Expression embedding and input representation
  * Performer-based encoder
  * Pre-training and fine-tuning strategies
- Data Processing
  * Dataset selection
  * Preprocessing steps

### 3. Results
- Multi-dataset Performance Evaluation
  * Cross-validation within datasets
  * Cross-dataset batch effect analysis
  * Impact of class imbalance
  * Novel cell type discovery
- Model Interpretability Analysis
  * Attention weight visualization
  * Enrichment analysis

### 4. Discussion
- Model Advantages
  * Strong modeling capability
  * Robustness to batch effects
  * Interpretability
- Limitations and Future Directions

## ğŸ”¬ Technical Details
### Algorithm Framework
1. Input Representation
   - Gene expression vector discretization
   - Gene embedding using pre-trained gene2vec
   - Expression embedding integration

2. Model Architecture
   - Performer-based encoder
   - Multi-head self-attention mechanism
   - Task-specific output layers

3. Training Strategy
   - Pre-training Phase:
     * Self-supervised learning on unlabeled data
     * Masked expression value prediction
   - Fine-tuning Phase:
     * Supervised learning for cell type annotation
     * End-to-end training with classification objective

### Implementation Details
- Language: Python
- Framework: PyTorch
- Key Packages:
  * Performer
  * scanpy
  * anndata
  * gene2vec

## ğŸ“Š Evaluation
### Baseline Models
1. Marker-based Methods:
   - SCINA
   - Garnett
   - scSorter

2. Correlation-based Methods:
   - Seurat
   - SingleR
   - CellID
   - scmap

3. Supervised Learning Methods:
   - scNym
   - SciBet

### Evaluation Metrics
- Accuracy
- Macro F1-score
- Confusion matrix

### Datasets
1. Zheng68K: Human PBMC cells
2. Pancreas datasets: Baron, Muraro, Segerstolpe, Xin
3. MacParland: Human liver tissue
4. Heart dataset
5. Lung dataset
6. Human Cell Atlas

## ğŸ’­ Critical Analysis
### Strengths
1. Strong performance across diverse datasets
2. Robust to batch effects and class imbalance
3. Interpretable attention mechanism
4. Efficient computation with Performer architecture

### Limitations
1. Expression embedding methodology
2. Gene interaction modeling
3. Pre-training masking strategy

### Future Directions
1. Improved expression embedding methods
2. Enhanced gene interaction modeling
3. Optimized pre-training strategies
4. Extension to other downstream tasks

## ğŸ“Œ Key Takeaways
1. Transformer architecture can be effectively adapted for scRNA-seq analysis
2. Pre-training on large-scale data improves cell type annotation
3. Attention mechanism provides biological interpretability
4. Model shows strong generalization across different tissues

## ğŸ’¡ Implementation Insights
1. Data Processing:
   - Discretization of gene expression values
   - Integration of gene embeddings
   - Batch effect handling

2. Training Tips:
   - Pre-training data selection
   - Fine-tuning strategies
   - Hyperparameter optimization



## Algorithm Framework

## ğŸ“‹ Model Overview
![scBERT Model Overview](../../../paper-figures/scBERT_overview.png)

a. æ— æ ‡ç­¾æ•°æ®çš„è‡ªç›‘ç£å­¦ä¹ å’Œç‰¹å®šä»»åŠ¡æ•°æ®çš„å¾®è°ƒã€‚åœ¨è‡ªç›‘ç£é¢„è®­ç»ƒé˜¶æ®µï¼Œä»PanglaoDBæ”¶é›†æ— æ ‡ç­¾æ•°æ®ã€‚æ©ç è¡¨è¾¾åµŒå…¥å’ŒåŸºå› åµŒå…¥ä½œä¸ºè¾“å…¥è¢«æ·»åŠ ï¼Œç„¶åè¾“å…¥åˆ°Performerå—ä¸­ã€‚é‡æ„å™¨ç”¨äºç”Ÿæˆè¾“å‡ºã€‚ç”¨äºæ©ç åŸºå› çš„è¾“å‡ºç”¨äºè®¡ç®—é‡æ„æŸå¤±ã€‚åœ¨ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œç‰¹å®šä»»åŠ¡çš„scRNA-seqæ•°æ®è¢«è¾“å…¥åˆ°é¢„è®­ç»ƒçš„ç¼–ç å™¨ä¸­ã€‚è¾“å‡ºè¡¨ç¤ºéšåé€šè¿‡ä¸€ç»´å·ç§¯å±‚å’Œåˆ†ç±»å™¨ç”Ÿæˆç»†èƒç±»å‹é¢„æµ‹ã€‚
ä»£è¡¨å…ƒç´ é€ä¸€ç›¸åŠ ã€‚Performerç¼–ç å™¨æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µæ¨¡å‹å…±äº«çš„ç»„ä»¶ã€‚é‡æ„å™¨å’Œåˆ†ç±»å™¨åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­åˆ†åˆ«ç‹¬ç«‹ä½¿ç”¨ã€‚

b. scBERTåµŒå…¥çš„ç¤ºæ„å›¾ã€‚é¢„å¤„ç†çš„scRNA-seqæ•°æ®é¦–å…ˆè½¬æ¢ä¸ºç¦»æ•£åŒ–è¡¨è¾¾ï¼Œç„¶åéšæœºæ©ç éé›¶è¡¨è¾¾ã€‚ä»¥ç¬¬ä¸€ä¸ªåŸºå› ä¸ºä¾‹ï¼ŒåŸºå› åµŒå…¥EG1ï¼ˆæ¥è‡ªgene2vecçš„åŸºå› èº«ä»½åµŒå…¥è½åœ¨ç¬¬ä¸€ä¸ªç®±ä¸­ï¼‰å’Œè¡¨è¾¾åµŒå…¥EB2ï¼ˆåŸºå› è¡¨è¾¾è½åœ¨ç¬¬äºŒä¸ªç®±ä¸­å¹¶è½¬æ¢ä¸ºä¸EG1ç›¸åŒçš„ç»´åº¦ï¼‰ç›¸åŠ å¹¶è¾“å…¥åˆ°scBERTä¸­ä»¥ç”ŸæˆåŸºå› çš„è¡¨ç¤ºã€‚è¿™äº›è¡¨ç¤ºç„¶åç”¨äºé¢„è®­ç»ƒæˆ–å¾®è°ƒã€‚

æ ¹æ®è®ºæ–‡çš„æ–¹æ³•éƒ¨åˆ†,å¯ä»¥æ€»ç»“å‡ºscBERTçš„ç®—æ³•æ¡†æ¶å¦‚ä¸‹:

1. è¾“å…¥è¡¨ç¤º
- åŸºå› è¡¨è¾¾å‘é‡:å°†æ¯ä¸ªç»†èƒçš„åŸå§‹åŸºå› è¡¨è¾¾é‡æ˜ å°„ä¸ºä¸€ä¸ªç¦»æ•£çš„è¡¨è¾¾å‘é‡,ç±»ä¼¼äºNLPä¸­çš„è¯è¢‹æ¨¡å‹
- åŸºå› åµŒå…¥å‘é‡:åˆ©ç”¨é¢„è®­ç»ƒçš„gene2vecæ¨¡å‹è·å¾—æ¯ä¸ªåŸºå› çš„åˆ†å¸ƒå¼è¡¨ç¤º,æ•æ‰åŸºå› é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§
2. æ¨¡å‹ç»“æ„
- ç¼–ç å™¨:ä½¿ç”¨å¤šå±‚Transformerçš„å˜ä½“Performerä½œä¸ºä¸»è¦çš„ç¼–ç å™¨ç»“æ„,å¯ä»¥å¤„ç†è¶…è¿‡16,000ä¸ªåŸºå› çš„è¾“å…¥
- æ³¨æ„åŠ›æœºåˆ¶:ä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥å­¦ä¹ åŸºå› ä¹‹é—´çš„äº¤äº’æ¨¡å¼å’Œä¾èµ–å…³ç³»
- è¾“å‡ºå±‚:æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡çš„ä¸åŒ,ä½¿ç”¨ä¸åŒçš„è¾“å‡ºå±‚å’ŒæŸå¤±å‡½æ•°(å¦‚åˆ†ç±»å±‚å’Œäº¤å‰ç†µæŸå¤±)
3. é¢„è®­ç»ƒé˜¶æ®µ
- æ•°æ®:ä½¿ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾çš„å•ç»†èƒè½¬å½•ç»„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒ
- ç›®æ ‡:é€šè¿‡è‡ªç›‘ç£çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡,è®©æ¨¡å‹å­¦ä¹ åˆ°é€šç”¨çš„åŸºå› è¡¨è¾¾æ¨¡å¼å’Œè°ƒæ§å…³ç³»
- è¿‡ç¨‹:éšæœºé®æŒ¡éƒ¨åˆ†éé›¶è¡¨è¾¾å€¼,ç„¶åè®­ç»ƒæ¨¡å‹æ ¹æ®å…¶ä»–åŸºå› çš„è¡¨è¾¾æ¥é¢„æµ‹è¢«é®æŒ¡çš„è¡¨è¾¾å€¼
4. å¾®è°ƒé˜¶æ®µ
- æ•°æ®:åœ¨å…·æœ‰ç»†èƒç±»å‹æ³¨é‡Šçš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ
- ç›®æ ‡:é€šè¿‡æœ‰ç›‘ç£çš„ç»†èƒç±»å‹åˆ†ç±»ä»»åŠ¡,è®©æ¨¡å‹é€‚åº”ç‰¹å®šçš„ç»†èƒç±»å‹æ³¨é‡Šé—®é¢˜
- è¿‡ç¨‹:å°†é¢„è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹å‚æ•°ä½œä¸ºåˆå§‹åŒ–,ç„¶åæ ¹æ®ç»†èƒç±»å‹æ ‡ç­¾è¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒ
5. æ¨ç†é˜¶æ®µ
- æ•°æ®:åœ¨æ–°çš„æµ‹è¯•æ•°æ®ä¸Šè¿›è¡Œç»†èƒç±»å‹æ³¨é‡Š
- è¿‡ç¨‹:å°†ç»†èƒçš„åŸºå› è¡¨è¾¾è¾“å…¥åˆ°è®­ç»ƒå¥½çš„scBERTæ¨¡å‹ä¸­,å¾—åˆ°æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ
- åå¤„ç†:æ ¹æ®é¢„æµ‹æ¦‚ç‡å¯¹ç»†èƒè¿›è¡Œæ³¨é‡Š,å¹¶å¯ä»¥è®¾ç½®é˜ˆå€¼ä»¥è¯†åˆ«æ–°çš„æœªçŸ¥ç»†èƒç±»å‹

æ€»çš„æ¥è¯´,scBERTçš„ç®—æ³•æ¡†æ¶ç»§æ‰¿äº†BERTçš„é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼,å¹¶åœ¨è¾“å…¥è¡¨ç¤ºå’Œç¼–ç å™¨ç»“æ„ä¸Šè¿›è¡Œäº†é’ˆå¯¹æ€§çš„æ”¹è¿›,ä»¥é€‚åº”å•ç»†èƒè½¬å½•ç»„æ•°æ®çš„ç‰¹ç‚¹ã€‚åŒæ—¶,scBERTå……åˆ†åˆ©ç”¨äº†è‡ªç›‘ç£å­¦ä¹ å’Œè¿ç§»å­¦ä¹ çš„æ€æƒ³,é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šçš„é¢„è®­ç»ƒæ¥å­¦ä¹ é€šç”¨çš„åŸºå› è¡¨è¾¾æ¨¡å¼,å†é€šè¿‡åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„å¾®è°ƒæ¥æå–ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾è¡¨ç¤º,ä»è€Œå®ç°äº†é«˜æ•ˆã€å‡†ç¡®ã€ç¨³å¥çš„ç»†èƒç±»å‹æ³¨é‡Šã€‚


#### å°†æ¯ä¸ªç»†èƒçš„åŸå§‹åŸºå› è¡¨è¾¾é‡æ˜ å°„ä¸ºä¸€ä¸ªç¦»æ•£çš„è¡¨è¾¾å‘é‡
    æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªå¨å¸ˆ,æ­£åœ¨åˆ†æä¸åŒèœå“çš„é…æ–¹ã€‚æ¯ä¸ªèœå“å°±ç›¸å½“äºä¸€ä¸ªç»†èƒ,è€Œæ¯ç§é…æ–™å°±ç›¸å½“äºä¸€ä¸ªåŸºå› ã€‚é…æ–™çš„ç”¨é‡å°±ç›¸å½“äºåŸºå› çš„è¡¨è¾¾é‡ã€‚

    åŸå§‹æ•°æ®:
    å‡è®¾ä½ æœ‰ä¸‰é“èœ,æ¯é“èœæœ‰å››ç§ä¸»è¦é…æ–™(ç›ã€ç³–ã€é†‹ã€è¾£æ¤’)çš„ç”¨é‡(å•ä½:å…‹):

    1. èœA: ç›(2.3), ç³–(15.7), é†‹(8.1), è¾£æ¤’(0.5)
    2. èœB: ç›(1.8), ç³–(5.2), é†‹(12.4), è¾£æ¤’(3.7)
    3. èœC: ç›(3.1), ç³–(0.3), é†‹(6.9), è¾£æ¤’(0)

    å°†åŸå§‹ç”¨é‡æ˜ å°„ä¸ºç¦»æ•£çš„è¡¨è¾¾å‘é‡:
    ä¸ºäº†ç®€åŒ–åˆ†æ,ä½ å†³å®šå°†ç”¨é‡åˆ†ä¸ºå››ä¸ªç­‰çº§:æ— (0)ã€å°‘é‡(1)ã€ä¸­ç­‰(2)ã€å¤§é‡(3)ã€‚å…·ä½“çš„æ˜ å°„è§„åˆ™å¦‚ä¸‹:

    - 0å…‹: 0 (æ— )
    - 0.1-2å…‹: 1 (å°‘é‡)
    - 2.1-10å…‹: 2 (ä¸­ç­‰)
    - 10å…‹ä»¥ä¸Š: 3 (å¤§é‡)

    åº”ç”¨è¿™ä¸ªè§„åˆ™å,ä½ å¾—åˆ°äº†ç¦»æ•£åŒ–çš„è¡¨è¾¾å‘é‡:

    1. èœA: [2, 3, 2, 1]
    2. èœB: [1, 2, 3, 2]
    3. èœC: [2, 1, 2, 0]

    è¿™æ ·,ä½ å°±å°†æ¯é“èœ(ç»†èƒ)çš„åŸå§‹é…æ–™ç”¨é‡(åŸºå› è¡¨è¾¾é‡)æ˜ å°„ä¸ºäº†ä¸€ä¸ªç¦»æ•£çš„è¡¨è¾¾å‘é‡ã€‚

    è¿™ç§ç¦»æ•£åŒ–çš„å¥½å¤„æ˜¯:

    1. ç®€åŒ–æ•°æ®: ä¸å†éœ€è¦å¤„ç†ç²¾ç¡®çš„å°æ•°å€¼,ä½¿åˆ†ææ›´åŠ é«˜æ•ˆã€‚
    2. é™ä½å™ªå£°: å°çš„æµ‹é‡è¯¯å·®ä¸ä¼šå½±å“æœ€ç»ˆçš„åˆ†ç±»ã€‚
    3. çªå‡ºå·®å¼‚: æ›´å®¹æ˜“çœ‹å‡ºé…æ–™ä½¿ç”¨é‡çš„ä¸»è¦åŒºåˆ«ã€‚
    4. æ ‡å‡†åŒ–: ä¸åŒèœå“ä¹‹é—´æ›´å®¹æ˜“æ¯”è¾ƒã€‚

    åœ¨å®é™…çš„å•ç»†èƒRNAæµ‹åºæ•°æ®ä¸­,è¿™ä¸ªè¿‡ç¨‹ä¼šåº”ç”¨åˆ°æˆåƒä¸Šä¸‡çš„åŸºå› ,ä½†åŸç†æ˜¯ç›¸åŒçš„ã€‚scBERTé€šè¿‡è¿™ç§æ–¹å¼å°†è¿ç»­çš„åŸºå› è¡¨è¾¾æ•°æ®è½¬æ¢ä¸ºç¦»æ•£çš„è¡¨è¾¾å‘é‡,ä¸ºåç»­çš„åˆ†ææä¾›äº†ä¸€ä¸ªæ›´åŠ ç»“æ„åŒ–å’Œæ˜“äºå¤„ç†çš„è¾“å…¥æ ¼å¼ã€‚
   
## Baseline Model, Evaluation Metrics, and Datasets

1. Baseline models
æ–‡ç« å°†scBERTä¸ä»¥ä¸‹å‡ ç±»ç»†èƒç±»å‹æ³¨é‡Šæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒ:
    1). åŸºäºmarkeråŸºå› çš„æ–¹æ³•:SCINAã€Garnettã€scSorter
    2). åŸºäºç›¸å…³æ€§çš„æ–¹æ³•:Seuratã€SingleRã€CellIDã€scmap
    3). åŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•:scNymã€SciBet

2. Evaluation metrics
ä¸ºäº†å…¨é¢è¯„ä¼°å„ç§æ–¹æ³•åœ¨ç»†èƒç±»å‹æ³¨é‡Šä»»åŠ¡ä¸Šçš„æ€§èƒ½,æ–‡ç« ä½¿ç”¨äº†ä»¥ä¸‹è¯„ä¼°æŒ‡æ ‡:
    1). Accuracy:è¡¡é‡æ•´ä½“æ³¨é‡Šå‡†ç¡®ç‡
    2). Macro F1-score:è¡¡é‡åœ¨ä¸åŒç±»åˆ«ä¸Šçš„å¹³å‡æ€§èƒ½,å¯¹ç±»åˆ«ä¸å¹³è¡¡æ›´ä¸ºç¨³å¥
    3). Confusion matrix:å±•ç¤ºä¸åŒç±»åˆ«ä¹‹é—´çš„é”™è¯¯åˆ†ç±»æƒ…å†µ,æœ‰åŠ©äºåˆ†ææ¨¡å‹çš„é”™è¯¯æ¨¡å¼

3. Datasets
æ–‡ç« ä½¿ç”¨äº†å¤šä¸ªå…¬å¼€çš„å•ç»†èƒRNAæµ‹åºæ•°æ®é›†æ¥è¯„ä¼°scBERTå’Œå…¶ä»–æ–¹æ³•,ä¸»è¦åŒ…æ‹¬:
    1). Zheng68Kæ•°æ®é›†:äººç±»å¤–å‘¨è¡€å•ä¸ªæ ¸ç»†èƒ(PBMC)æ•°æ®,åŒ…å«11ç§ç»†èƒç±»å‹,å¹¿æ³›ç”¨äºç»†èƒç±»å‹æ³¨é‡Šä»»åŠ¡çš„åŸºå‡†æµ‹è¯•
    2). èƒ°è…ºæ•°æ®é›†:Baronã€Muraroã€Segerstolpeå’ŒXin,æ¥è‡ªäººç±»èƒ°è…ºçš„ä¸åŒå•ç»†èƒæµ‹åºå¹³å°çš„æ•°æ®
    3). MacParlandæ•°æ®é›†:æ¥è‡ªäººè‚è„ç»„ç»‡çš„å•ç»†èƒè½¬å½•ç»„æ•°æ®
    4). å¿ƒè„æ•°æ®é›†:ä¸€ä¸ªå¤§è§„æ¨¡çš„å¿ƒè„å•ç»†èƒæ•°æ®é›†ç”¨äºé¢„è®­ç»ƒ,Tuckeræ•°æ®é›†ç”¨äºå¾®è°ƒå’Œæµ‹è¯•
    5). è‚ºæ•°æ®é›†:æ¥è‡ªæ–°å† è‚ºç‚ç›¸å…³çš„äººç±»è‚ºç»„ç»‡çš„å•ç»†èƒè½¬å½•ç»„æ•°æ®
    6). äººç±»ç»†èƒå›¾è°±æ•°æ®é›†:è¦†ç›–15ä¸ªä¸»è¦å™¨å®˜çš„äººä½“å•ç»†èƒæ•°æ®é›†

è¿™äº›æ•°æ®é›†æ¶µç›–äº†ä¸åŒç‰©ç§ã€ç»„ç»‡å™¨å®˜ã€æµ‹åºå¹³å°å’Œç»†èƒç±»å‹,æœ‰åŠ©äºå…¨é¢è¯„ä¼°scBERTåœ¨å„ç§åœºæ™¯ä¸‹çš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚
## Computing Language, Tools, Packages, and Resources
1. Computing language
- Python

2. Tools and packages
    - PyTorch:ä¸€ä¸ªæµè¡Œçš„æ·±åº¦å­¦ä¹ æ¡†æ¶,ç”¨äºå®ç°scBERTæ¨¡å‹çš„ä¸»è¦æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹ã€‚
    - Performer:Transformerçš„ä¸€ä¸ªå˜ä½“,ç”¨äºå¤„ç†é•¿åºåˆ—è¾“å…¥,æé«˜scBERTåœ¨å¤„ç†å¤§è§„æ¨¡å•ç»†èƒæ•°æ®æ—¶çš„è®¡ç®—æ•ˆç‡ã€‚
    - scanpy:ä¸€ä¸ªç”¨äºå•ç»†èƒRNAæµ‹åºæ•°æ®åˆ†æçš„Pythonå·¥å…·åŒ…,æ–‡ç« ä½¿ç”¨å…¶è¿›è¡Œæ•°æ®é¢„å¤„ç†å’Œè´¨é‡æ§åˆ¶ã€‚
    - anndata:ä¸€ä¸ªç”¨äºå­˜å‚¨å’Œæ“ä½œå¸¦æ³¨é‡Šçš„å¤šç»´æ•°ç»„çš„PythonåŒ…,å¸¸ç”¨äºå•ç»†èƒæ•°æ®åˆ†æã€‚
    - gene2vec:ä¸€ä¸ªé¢„è®­ç»ƒçš„åŸºå› åµŒå…¥æ¨¡å‹,ç”¨äºæ•æ‰åŸºå› ä¹‹é—´çš„åŠŸèƒ½ç›¸ä¼¼æ€§ã€‚
    - SCINAã€Garnettã€scSorterã€Seuratã€SingleRã€CellIDã€scmapã€scNymã€SciBetç­‰:æ–‡ç« ä¸­ç”¨äºæ¯”è¾ƒçš„å…¶ä»–ç»†èƒç±»å‹æ³¨é‡Šæ–¹æ³•,è¿™äº›æ–¹æ³•çš„ä»£ç å¤§å¤šæ˜¯é€šè¿‡Ræˆ–PythonåŒ…çš„å½¢å¼æä¾›çš„ã€‚

3. Resources
    - PanglaoDBæ•°æ®åº“:ä¸€ä¸ªå¤§è§„æ¨¡çš„å•ç»†èƒRNAæµ‹åºæ•°æ®é›†åˆ,ç”¨äºscBERTæ¨¡å‹çš„é¢„è®­ç»ƒé˜¶æ®µã€‚
    - å…¶ä»–å…¬å¼€æ•°æ®é›†:å¦‚Zheng68Kã€èƒ°è…ºæ•°æ®é›†ã€MacParlandç­‰,ä¸»è¦æ¥è‡ªGEOã€ArrayExpressç­‰å…¬å…±æ•°æ®åº“,ç”¨äºæ¨¡å‹çš„å¾®è°ƒå’Œæµ‹è¯•ã€‚
    - CellMarkeræ•°æ®åº“:ä¸€ä¸ªæ‰‹å·¥æ³¨é‡Šçš„ç»†èƒç±»å‹æ ‡è®°åŸºå› æ•°æ®åº“,éƒ¨åˆ†å®éªŒä¸­ç”¨äºæä¾›markeråŸºå› çš„ä¿¡æ¯ã€‚
    - Enrichr:ä¸€ä¸ªåŸºå› å¯Œé›†åˆ†æçš„åœ¨çº¿å·¥å…·,ç”¨äºå¯¹scBERTå…³æ³¨çš„åŸºå› è¿›è¡ŒåŠŸèƒ½è§£é‡Šã€‚

![æµç¨‹å›¾](../../../paper-figures/2024-10-SSL-BenchmarkStudy-NatureMachineIntelligence_flowchart.png)
![æ¨¡å‹æ¶æ„](../../../paper-figures/2024-10-SSL-BenchmarkStudy-NatureMachineIntelligence_model_architechture.png)
